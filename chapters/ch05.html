<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Chapter 05 — Reliability, SLOs & SRE Practices</title>
  <meta name="description" content="Create meaningful SLIs/SLOs with error budgets, reduce toil via automation and runbooks, run incidents and blameless postmortems, and test reliability with chaos and safe rollouts.">
  <meta property="og:title" content="Chapter 05 — Reliability, SLOs & SRE Practices">
  <meta property="og:description" content="From goals to operations: user‑centric SLIs/SLOs, error budgets, incident management, runbooks, chaos experiments, load testing, canaries, and rollbacks.">
  <meta property="og:type" content="article">
  <base href="../">
  <link rel="stylesheet" href="styles/theme.css">
  <script src="scripts/app.js" defer></script>
</head>
<body>
  <a class="skip-link" href="#main">Skip to main content</a>

  <header class="app-nav">
    <div class="container inner">
      <div class="brand">Intermediate System Design</div>
      <button class="toggle js-nav-toggle" aria-expanded="false" aria-controls="top-menu">☰ Menu</button>
      <nav id="top-menu" class="menu" aria-label="Primary">
        <a href="index.html">Home</a>
        <a href="chapters/appendix.html">Appendix</a>
        <a href="chapters/glossary.html">Glossary</a>
      </nav>
    </div>
  </header>

  <main id="main" class="container fade-in">
    <section class="page-hero" id="5-hero">
      <div class="meta"><span class="badge badge-primary">Chapter 05</span></div>
      <h1>Reliability, SLOs &amp; SRE Practices</h1>
      <p class="abstract">Reliability is not an adjective; it is an engineered budget. In this chapter you will translate product goals into user‑visible SLIs, choose SLO targets, and manage an error budget that balances innovation with stability. You will learn to eliminate toil with runbooks and automation, run incidents with clarity and calm, write blameless postmortems that actually change systems, and validate reliability with chaos experiments, load tests, and safe rollout patterns.</p>
    </section>

    <section class="section" id="5-1-slo-sli-foundations">
      <h2>5.1 SLO/SLI foundations</h2>
      <p><strong>Why it matters.</strong> Teams often ship features quickly until an outage resets trust. SLIs and SLOs tie engineering to user value and provide a measurable throttle for change.</p>

      <h3 id="5-1-1-choosing-slis">5.1.1 Choosing SLIs that reflect user experience</h3>
      <p><strong>Plain.</strong> An <em>SLI</em> is a number you measure. It should match what the user feels: latency to complete a task, success rate, data freshness, or correctness. <strong>Formal.</strong> An SLI is a ratio over a time window; e.g., “the proportion of requests to <code>POST /checkout</code> completed under 300 ms and HTTP 2xx in the last 28 days.” <strong>Pitfall.</strong> Host‑level metrics (CPU, queue depth) are diagnostics, not SLIs. They can be green while the user experience is red.</p>
      <ul>
        <li><strong>Availability SLI:</strong> 1 − (error responses / total requests) for <em>user‑initiated</em> actions, not background jobs.</li>
        <li><strong>Latency SLI:</strong> P95 and P99 of <em>end‑to‑end</em> latency measured at the client and corroborated on the server with tracing.</li>
        <li><strong>Freshness SLI:</strong> age of data powering a page (e.g., product price staleness) at P99, not average.</li>
        <li><strong>Correctness SLI:</strong> proportion of results matching the ground truth or invariants (e.g., order totals consistent with line items).</li>
      </ul>
      <p><strong>Analogy.</strong> If a restaurant’s oven is hot (CPU) but orders take 40 minutes (latency), the experience is bad. Measure <em>food‑to‑table time</em> (end‑to‑end), not oven temperature.</p>

      <p>Compare: <strong>Good vs weak SLIs</strong></p>
      <table class="table">
        <thead><tr><th>Goal</th><th>Weak SLI</th><th>Better SLI</th><th>Why better</th></tr></thead>
        <tbody>
          <tr><td>Fast checkout</td><td>DB CPU &lt; 70%</td><td>P95 client‑perceived checkout latency</td><td>Captures network, app, and DB together</td></tr>
          <tr><td>Fresh prices</td><td>Cache hit ratio</td><td>P99 price freshness age &lt; X sec</td><td>Directly reflects user‑visible staleness</td></tr>
          <tr><td>Reliable chat</td><td>Broker latency</td><td>Send‑to‑receive latency at P95/P99</td><td>End‑to‑end delivery, not one hop</td></tr>
        </tbody>
      </table>

      <h3 id="5-1-2-slos-error-budgets">5.1.2 Defining SLOs and calculating error budgets</h3>
      <p><strong>Plain.</strong> An <em>SLO</em> is a target for your SLI. The <em>error budget</em> is 1 − SLO: the amount of failure you’re willing to tolerate over a period. <strong>Formal.</strong> With a 99.9% availability SLO over 30 days, your error budget is ~43 minutes of non‑compliance. You decide when to spend it (e.g., canarying a risky feature) and when to stop (freeze deploys when burn rate exceeds a threshold).</p>
      <p><strong>Worked example — Catalog.</strong> SLI: P95 edge render time for <code>/item</code>. SLO: ≤ 120 ms over 30 days. Error budget: the 5% of requests beyond 120 ms. <em>Policy:</em> if burn rate &gt; 2× over 6 hours, freeze non‑urgent changes and investigate. If burn &gt; 4×, rollback the last change and enable a reduced‑content variant to shed load.</p>
      <p><strong>Worked example — Chat.</strong> SLI: P95 send‑to‑receive latency ≤ 150 ms; SLO: 95% compliance. When introducing presence fan‑out optimization, you choose to spend budget during peak hours while monitoring delivery failure rate and user‑reported lag. If budget is consumed, you disable the optimization and file a follow‑up design.</p>

      <p>Compare: <strong>Error budget governance patterns</strong></p>
      <table class="table">
        <thead><tr><th>Pattern</th><th>When to use</th><th>Pros</th><th>Cons</th></tr></thead>
        <tbody>
          <tr><td>Freeze on burn</td><td>Volatile systems; rapid regressions</td><td>Fast brake; simple rule</td><td>May block low‑risk work</td></tr>
          <tr><td>Change review</td><td>Steady burn; complex systems</td><td>Nuanced; allocates budget</td><td>Requires discipline/committee</td></tr>
          <tr><td>Team quotas</td><td>Many teams changing one platform</td><td>Aligns incentives; fair sharing</td><td>Overhead to track/adjust</td></tr>
        </tbody>
      </table>
      <p><em>Takeaway:</em> Error budgets are a <em>throttle</em>. They let you choose speed with eyes open rather than crash by surprise.</p>
    </section>

    <section class="section" id="5-2-toil-automation">
      <h2>5.2 Eliminating toil & automation</h2>
      <p><strong>Why it matters.</strong> Toil is repetitive, manual work that scales linearly with service size. It steals time from engineering and increases error risk. Reducing toil improves both reliability and developer morale.</p>

      <h3 id="5-2-1-runbooks-playbooks">5.2.1 Runbooks, playbooks, and runbook automation</h3>
      <p><strong>Plain.</strong> A <em>runbook</em> is a step‑by‑step guide to diagnose and mitigate a class of incidents. A <em>playbook</em> orchestrates multiple runbooks and communication steps for a scenario (e.g., partial region outage). <strong>Formal.</strong> Effective runbooks include prerequisites, decision trees, exact commands with example outputs, metrics to watch, and explicit rollback steps. <strong>Pitfall.</strong> Runbooks decay unless rehearsed; treat them like code with owners, reviews, and simulations.</p>
      <ul>
        <li><strong>Minimum runbook sections:</strong> Summary, Preconditions, Steps, Verification, Rollback, Owner, Last‑reviewed.</li>
        <li><strong>Evidence rule:</strong> Include screenshots or links to dashboards; paste sample log queries. No vague steps like “check logs.”</li>
        <li><strong>Automation:</strong> Promote frequent steps to scripts or ChatOps commands with guardrails and dry‑run modes.</li>
      </ul>

      <h3 id="5-2-2-reducing-manual-steps">5.2.2 Reduce manual steps with safe automation</h3>
      <p><strong>Plain.</strong> Automate high‑frequency, low‑ambiguity tasks first: restart a service, flip a feature flag, drain a node, roll a canary back. <strong>Formal.</strong> Use <em>least privilege</em>, <em>approval gates</em> for destructive operations, and <em>idempotent</em> scripts that can be retried safely. <strong>Pitfall.</strong> “One‑button fixes” that hide context can make things worse under stress; design CLI or ChatOps commands to <em>explain what they’ll do</em> before they do it.</p>

      <p>Compare: <strong>Automation candidates</strong></p>
      <table class="table">
        <thead><tr><th>Task</th><th>Frequency</th><th>Ambiguity</th><th>Automate?</th><th>Notes</th></tr></thead>
        <tbody>
          <tr><td>Clear stuck jobs</td><td>Weekly</td><td>Low</td><td>Yes</td><td>Include safety threshold; dry‑run</td></tr>
          <tr><td>Scale read replicas</td><td>Monthly</td><td>Medium</td><td>Partial</td><td>Script creation + human approval</td></tr>
          <tr><td>Fail over region</td><td>Rare</td><td>High</td><td>No (practice)</td><td>Runbook + game day; simulate</td></tr>
        </tbody>
      </table>
      <p><em>Takeaway:</em> Automate <em>decisions</em> last. Automate <em>actions</em> first, with clear preconditions and rollbacks.</p>
    </section>

    <section class="section" id="5-3-incidents-postmortems">
      <h2>5.3 Incident management & postmortems</h2>
      <p><strong>Why it matters.</strong> Incidents are inevitable; chaos is optional. A shared model for severity, roles, and communication reduces mean time to mitigation and preserves trust with users and stakeholders.</p>

      <h3 id="5-3-1-incident-lifecycle">5.3.1 Incident lifecycle and communication</h3>
      <p><strong>Plain.</strong> An incident is user‑visible degradation or risk thereof. <strong>Formal.</strong> The lifecycle is: <em>detect → declare → triage → mitigate → resolve → learn</em>. <strong>Pitfall.</strong> Late declarations create confusion and untracked damage; declare early with severity and scope, then update as you learn.</p>
      <ul>
        <li><strong>Severity rubric (example):</strong> SEV‑1 (widespread outage), SEV‑2 (major feature degraded), SEV‑3 (minor feature degraded), SEV‑4 (near miss).</li>
        <li><strong>Roles:</strong> <em>Incident Commander</em> (decision owner), <em>Operations</em> (mitigation), <em>Communications</em> (status updates), <em>Scribe</em> (timeline), <em>Subject Matter Experts</em> as needed.</li>
        <li><strong>Channels:</strong> one command channel, one updates channel; all decisions flow through the commander; avoid side threads with conflicting instructions.</li>
      </ul>

      <p>Compare: <strong>Communication templates</strong></p>
      <table class="table">
        <thead><tr><th>Audience</th><th>Cadence</th><th>Template</th></tr></thead>
        <tbody>
          <tr><td>Internal engineers</td><td>Every 15–30 min</td><td>“SEV‑2: Checkout errors up 8%. Suspected deploy 42. Mitigation: rollback in progress. Next update 20:30 IST.”</td></tr>
          <tr><td>Customer‑facing status page</td><td>30–60 min</td><td>“We are investigating elevated checkout errors for some users. We’ve rolled back a recent change and are monitoring. Next update at 20:30 IST.”</td></tr>
          <tr><td>Executives/Support</td><td>Milestones</td><td>“Impact: 12% of checkouts failed for 22 minutes. Cause: config error. Action: hotfix deployed; safeguards added.”</td></tr>
        </tbody>
      </table>

      <p><strong>Worked example — Split‑brain cache.</strong> A failed network partition created two cache clusters serving divergent data. Symptoms: price mismatches; elevated checkout rejections. <em>Mitigation:</em> force read‑through for price endpoints; disable write‑back temporarily; drain and rejoin the cluster; add health checks and quorum writes. <em>Follow‑ups:</em> introduce <em>fencing tokens</em> to prevent stale writers and a rapid alert when read‑write divergence exceeds a threshold.</p>

      <h3 id="5-3-2-blameless-postmortems">5.3.2 Blameless postmortems and corrective action</h3>
      <p><strong>Plain.</strong> Blameless means we explain <em>how</em> the system made a mistake easy, not <em>who</em> made it. <strong>Formal.</strong> A postmortem records timeline, impact, detection gaps, contributing factors, what worked, and prioritized actions. <strong>Pitfall.</strong> Action lists without owners or dates do not change future outcomes.</p>
      <ul>
        <li><strong>Five whys with care:</strong> stop when you hit an organizational constraint (process, tooling) you can change; avoid drifting into philosophy.</li>
        <li><strong>Actions must be testable:</strong> “add dashboard X,” “write runbook Y,” “build circuit breaker Z,” each with an owner and due date.</li>
        <li><strong>Budget for learning:</strong> reserve capacity to complete postmortem actions; track completion rate like feature work.</li>
      </ul>
      <p><strong>Worked example — Incorrect feature flag default.</strong> A new feature shipped with default=on in production config. Impact: P95 latency up 60% for 40 minutes. <em>Actions:</em> (1) Add <em>safe default</em> policy (off unless proven). (2) Enforce a two‑person review for production flag changes. (3) Add a <em>global kill switch</em> path tested weekly.</p>
    </section>

    <section class="section" id="5-4-testing-reliability">
      <h2>5.4 Testing for reliability</h2>
      <p><strong>Why it matters.</strong> Reliability emerges when systems face turbulence. Testing introduces turbulence in controlled ways so you can discover weak links in daylight rather than at 3 AM.</p>

      <h3 id="5-4-1-chaos-fault-injection">5.4.1 Chaos experiments and fault injection</h3>
      <p><strong>Plain.</strong> Chaos engineering introduces controlled failure: killing processes, adding latency, exhausting resources, or breaking network links. <strong>Formal.</strong> An experiment states a <em>steady state hypothesis</em> (SLIs remain within bounds) and a <em>treatment</em> (fault) with scope and abort criteria. <strong>Pitfall.</strong> Running chaos without guardrails or observability creates self‑inflicted incidents.</p>
      <ul>
        <li><strong>Experiment template:</strong> Hypothesis → Fault → Scope (service/pod/region) → Abort conditions → Rollback plan → Owners → Results.</li>
        <li><strong>Common faults:</strong> +200 ms latency on dependency, drop 5% packets, kill 1 pod every minute, throttle disk IO to 5 MB/s, expire TLS cert in staging.</li>
        <li><strong>Safety:</strong> start in staging; then restricted production windows with traffic guards and error‑budget checks.</li>
      </ul>
      <p><strong>Worked example — Dependency latency spike.</strong> Hypothesis: “Checkout P95 &lt; 300 ms with +150 ms on the pricing service.” Treatment: inject latency in staging; then 5% prod traffic for 10 minutes behind a flag. <em>Result:</em> circuit breaker opened correctly; fallback price path served 2% of requests; SLO held. <em>Action:</em> lower breaker threshold and add a dashboard panel for fallback rate.</p>

      <h3 id="5-4-2-load-canary-rollouts">5.4.2 Load testing and canary rollouts</h3>
      <p><strong>Plain.</strong> Load tests reveal capacity limits and tail behaviors; canaries deploy changes to a small slice of traffic to reduce risk. <strong>Formal.</strong> A good load test defines arrival patterns (steady, spikes), data realism (cache warm/cold), and measurement (P95/P99 latency, error rates, resource saturation). <strong>Pitfall.</strong> Synthetic tests with toy data hide hot‑key and cache issues; always include “unfriendly” data and randomized TTLs.</p>
      <ul>
        <li><strong>Arrival models:</strong> open‑loop (constant RPS) vs closed‑loop (user think time). Use both: open‑loop for capacity, closed‑loop for UX realism.</li>
        <li><strong>Canary policy:</strong> start at 1–5%, watch SLI deltas, expand logarithmically (1→5→10→25→50→100%). Abort on predefined thresholds (e.g., +20% P95, +0.5% absolute error rate).</li>
        <li><strong>Rollbacks:</strong> implement as a first‑class operation; test weekly. Automate to reverse both code and config changes.</li>
      </ul>

      <p>Compare: <strong>Release strategies</strong></p>
      <table class="table">
        <thead><tr><th>Strategy</th><th>How it works</th><th>Pros</th><th>Cons</th><th>Use when</th></tr></thead>
        <tbody>
          <tr><td>Blue/Green</td><td>Two environments; switch traffic</td><td>Instant rollback; isolation</td><td>Infra cost; data sync complexity</td><td>Few, high‑stakes releases</td></tr>
          <tr><td>Canary</td><td>Gradual % rollout</td><td>Small blast radius; real traffic</td><td>Longer deploy; needs metrics</td><td>Frequent releases; SLI‑driven</td></tr>
          <tr><td>Feature flags</td><td>Behavior toggled at runtime</td><td>Per‑user control; experiments</td><td>Flag debt; config discipline</td><td>UI/UX changes; A/B tests</td></tr>
        </tbody>
      </table>

      <p><strong>Worked example — Canary gone wrong.</strong> A JSON schema change increased payload size, pushing responses over a CDN compression threshold. P95 rose 30% only for the canary cohort (mobile users with slow networks). <em>Fix:</em> add canary cohorts that mirror device/network diversity; track payload size as a first‑class metric; compress responses consistently across variants.</p>
    </section>

    <section class="section" id="5-5-mini-exercises">
      <h2>5.5 Mini‑exercises</h2>
      <ol>
        <li><strong>Define two SLIs and one SLO.</strong> For your core user journey, write precise SLI definitions and a 30‑day SLO. Include measurement location (client/server), percentile, and window. Add the alert you would create (threshold, duration).</li>
        <li><strong>Draft a runbook.</strong> Pick a likely incident (dependency latency) and write a 10‑step runbook with commands, dashboards, and rollback. Share it for review and schedule a rehearsal.</li>
        <li><strong>Design a chaos experiment.</strong> State the steady state, fault, scope, and abort conditions. Run it in staging and summarize results and changes you made.</li>
        <li><strong>Plan a canary.</strong> Choose ramp steps and abort criteria; list the exact SLI deltas you’ll monitor and who approves promotion to 100%.</li>
      </ol>
    </section>

    <section class="section" id="5-6-resources">
      <h2>Resources</h2>
      <ul>
        <li><strong>Google SRE Book &amp; Website</strong> — foundational guidance on SLIs/SLOs, error budgets, incidents, and postmortems. <em>Why:</em> proven practices for reliability at scale. <em>(Free)</em></li>
        <li><strong>Site Reliability Workbook</strong> — hands‑on playbooks for SLOs, alerting, and on‑call. <em>Why:</em> concrete templates and examples. <em>(Paid/Free excerpts)</em></li>
        <li><strong>AWS Well‑Architected — Reliability</strong> — questions and best practices for resilience. <em>Why:</em> decision lenses to avoid fragility. <em>(Free)</em></li>
        <li><strong>OpenTelemetry Docs</strong> — metrics, logs, traces standards. <em>Why:</em> consistent observability across services. <em>(Free)</em></li>
        <li><strong>Incident Response Guides (PagerDuty‑style)</strong> — role definitions and comms templates. <em>Why:</em> faster, calmer incidents. <em>(Free)</em></li>
      </ul>
    </section>

    <section class="section" id="5-7-recap-next">
      <h2>Recap &amp; Next Steps</h2>
      <ul>
        <li>You can design user‑centric SLIs, set SLOs with clear windows and percentiles, and govern change using error budgets.</li>
        <li>You can reduce toil with well‑maintained runbooks and safe automation, focusing on high‑frequency, low‑ambiguity tasks.</li>
        <li>You can run incidents with defined roles and communication cadences, and write blameless postmortems with actionable follow‑ups.</li>
        <li>You can validate resilience with chaos experiments, load testing, and safe rollout strategies (blue/green, canary, feature flags) and execute rollbacks confidently.</li>
      </ul>
      <p><strong>Next:</strong> Move on to <a href="chapters/ch06.html#6-hero">Chapter 6 — Consistency, Partitioning &amp; Concurrency</a>, where we examine the data‑consistency and concurrency controls that make distributed systems correct under partitions and contention.</p>
    </section>

    <nav class="next-prev">
      <a class="btn" rel="prev" href="chapters/ch04.html">← Previous</a>
      <a class="btn btn-primary" rel="next" href="chapters/ch06.html">Next →</a>
    </nav>

    <footer class="site-footer">
      <div class="container">
        <p class="muted">© 2025 BookBuilder. Built with vanilla HTML/CSS/JS. Dark theme.</p>
      </div>
    </footer>
  </main>

  <!--
  CHECKLIST (Chapter 05)
  - [x] /styles/theme.css + /scripts/app.js linked; <base> correct; no inline nav JS
  - [x] Canonical nav (Home / Appendix / Glossary only)
  - [x] Pager prev → ch04, next → ch06; ToC numbering matches
  - [x] Order: Hero → Numbered Sections → Resources → Recap
  - [x] ≥1,800 words of prose (body text)
  - [x] Images: none (no audit required)
  - [x] Head/meta complete; no TODOs
  -->
</body>
</html>

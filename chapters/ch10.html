<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Chapter 10 — Pitfalls, Next Steps & Optional Advanced Topics</title>
  <meta name="description" content="Avoid common intermediate‑level architecture mistakes, plan your next steps toward advanced topics like capacity planning and multi‑region, and sample optional deep dives in global consistency, streaming, and platform engineering.">
  <meta property="og:title" content="Chapter 10 — Pitfalls, Next Steps & Optional Advanced Topics">
  <meta property="og:description" content="A field guide to what goes wrong (and how to fix it), how to keep growing after the capstone, and three optional advanced topics with practical entry ramps.">
  <meta property="og:type" content="article">
  <base href="../">
  <link rel="stylesheet" href="styles/theme.css">
  <script src="scripts/app.js" defer></script>
</head>
<body>
  <a class="skip-link" href="#main">Skip to main content</a>

  <header class="app-nav">
    <div class="container inner">
      <div class="brand">Intermediate System Design</div>
      <button class="toggle js-nav-toggle" aria-expanded="false" aria-controls="top-menu">☰ Menu</button>
      <nav id="top-menu" class="menu" aria-label="Primary">
        <a href="index.html">Home</a>
        <a href="chapters/appendix.html">Appendix</a>
        <a href="chapters/glossary.html">Glossary</a>
      </nav>
    </div>
  </header>

  <main id="main" class="container fade-in">
    <section class="page-hero" id="10-hero">
      <div class="meta"><span class="badge badge-primary">Chapter 10</span></div>
      <h1>Pitfalls, Next Steps &amp; Optional Advanced Topics</h1>
      <p class="abstract">You now have the tools to design, justify, and run a production‑like system. This final chapter shows the most common ways good teams still get into trouble—and how to get out. Then we lay a practical path to the advanced tier: capacity planning, multi‑region, and cost discipline. Finally, we preview three optional deep dives—global consistency & geo‑replication, advanced streaming, and platform engineering—so you can choose where to invest next.</p>
    </section>

    <section class="section" id="10-1-pitfalls">
      <h2>10.1 Common pitfalls</h2>
      <p><strong>Why it matters.</strong> Most failures are not exotic; they are predictable results of ignoring ownership, operations, or user‑centric reliability. Naming the trap gives you leverage to avoid it.</p>

      <h3 id="10-1-1-over-splitting">10.1.1 Over‑splitting services before need</h3>
      <p><strong>Plain.</strong> Breaking a monolith into many tiny services too early makes everything slower: design, testing, releases, on‑call. <strong>Formal.</strong> <em>Premature decomposition</em> increases <em>coordination cost</em> (number of cross‑service contracts to maintain), <em>failure surface</em> (more hops and timeouts), and <em>operational overhead</em> (pipelines, dashboards, runbooks) without proportional benefit. <strong>Pitfall.</strong> Decomposing by CRUD tables (“user service,” “profile service,” “preferences service”) rather than cohesive capabilities that change together. A sure sign is lock‑step deploys across “independent” services.</p>
      <p><strong>Example.</strong> A team extracted notifications into three services (composer, scheduler, sender). Each release required choreography across all three to fix a single bug. Consolidating into one <em>Notifications</em> service with a background worker cut deploy time and resolved most incidents; a queue separated the UI thread from actual sends.</p>
      <p><strong>Analogy.</strong> If your kitchen is one cook and two burners, you don’t build a restaurant with seven stations. You first master a small menu and only split stations when each is constantly busy.</p>
      <p>Compare: <strong>Should we split?</strong></p>
      <table class="table">
        <thead><tr><th>Signal</th><th>Split now</th><th>Consolidate / wait</th><th>Why</th></tr></thead>
        <tbody>
          <tr><td>Change cadence</td><td>Different teams change independently</td><td>Same team always changes both</td><td>Split by cohesion of change, not nouns</td></tr>
          <tr><td>Runtime chat</td><td>Low QPS between services</td><td>High QPS, hot path needs both</td><td>Chatty split adds latency & failure modes</td></tr>
          <tr><td>Data invariants</td><td>Weak cross‑table invariants</td><td>Strong invariants across tables</td><td>Co‑locate strong invariants</td></tr>
          <tr><td>Ownership</td><td>Clear owners for each slice</td><td>Ownership unclear or shared</td><td>Shared ownership breeds outages</td></tr>
        </tbody>
      </table>
      <p><em>Action.</em> If two services always change together or chat on every request, merge them behind one deployable. Keep the API boundary if useful; re‑split later when evidence appears.</p>

      <h3 id="10-1-2-cost-observability">10.1.2 Ignoring operational cost and observability</h3>
      <p><strong>Plain.</strong> A design that looks elegant on a whiteboard can bankrupt your team’s time or budget when it meets reality. <strong>Formal.</strong> <em>Total cost of ownership</em> (TCO) = cloud spend + licenses + engineering time (toil, incident response, slow iteration) + risk (outages, security incidents). <strong>Pitfall.</strong> Optimizing for per‑request compute while ignoring <em>telemetry</em> costs, on‑call load, and slow developer loops.</p>
      <ul>
        <li><strong>Telemetry budgets.</strong> Set explicit ingestion and retention targets for logs and traces; use sampling (head/tail) and exemplars (OpenTelemetry, various vendors; Google SRE, 2016) to keep high‑signal detail without runaway indexing costs.</li>
        <li><strong>Cost aware patterns.</strong> Choose managed services when feature velocity is the bottleneck; switch to self‑managed only when stable workloads justify it (AWS Well‑Architected, 2025).</li>
        <li><strong>Observability first.</strong> If you can’t explain a regression within one hour using dashboards and traces, your next sprint starts with instrumentation work. Treat this policy as a <em>guardrail</em>.</li>
      </ul>
      <p>Compare: <strong>Observability investment vs hidden costs</strong></p>
      <table class="table">
        <thead><tr><th>Choice</th><th>Short‑term cost</th><th>Long‑term effect</th><th>When right</th><th>When wrong</th></tr></thead>
        <tbody>
          <tr><td>Full tracing on critical path</td><td>Higher ingest</td><td>Faster incident resolution; safer deploys</td><td>When SLOs tight, complex fan‑out</td><td>When you never read traces → sample instead</td></tr>
          <tr><td>Log everything</td><td>Exploding indexes</td><td>Searchable forensics</td><td>Short‑term during a spike</td><td>Default posture—use structure + sampling</td></tr>
          <tr><td>Managed broker</td><td>Higher invoice</td><td>Lower SRE toil</td><td>Feature‑constrained teams</td><td>When steady, predictable throughput + SRE time available</td></tr>
        </tbody>
      </table>
      <p><em>Action.</em> Write a one‑page telemetry budget and review it monthly like any other product budget. Alert on ingestion overshoot and cardinality explosions.</p>

      <h3 id="10-1-3-slos-mismatch">10.1.3 SLOs that don’t map to business outcomes</h3>
      <p><strong>Plain.</strong> Teams sometimes measure what’s easy (CPU, queue depth) instead of what users feel (latency to complete a task, success rate). <strong>Formal.</strong> An SLI is only valid if it correlates with user happiness and business outcomes; otherwise you are optimizing a proxy. <strong>Pitfall.</strong> “99.99% availability” on a health check while users can’t log in due to configuration or third‑party outages.</p>
      <p><strong>Worked example — Login SLO.</strong> Weak SLO: “Auth service 99.99% up.” Better: “95% of users can sign in within 400 ms, measured at the client with a valid token in hand.” Add a dependency budget: “Third‑party identity calls may consume at most 20% of the error budget; otherwise we switch to cached tokens or degrade MFA with clear messaging.”</p>
      <p>Compare: <strong>Weak vs strong SLO mapping</strong></p>
      <table class="table">
        <thead><tr><th>Goal</th><th>Weak SLI/SLO</th><th>Strong SLI/SLO</th><th>Why strong</th></tr></thead>
        <tbody>
          <tr><td>Checkout reliability</td><td>DB CPU &lt; 70%</td><td>95% of checkouts &lt; 300 ms with 2xx (client‑measured)</td><td>Captures user experience end‑to‑end</td></tr>
          <tr><td>Fresh analytics</td><td>Pipeline throughput</td><td>P99 dashboard freshness &lt; 120 s</td><td>Measures lag users see</td></tr>
          <tr><td>Chat delivery</td><td>Broker uptime</td><td>Send→receive P95 &lt; 150 ms</td><td>End‑to‑end delivery, not a hop</td></tr>
        </tbody>
      </table>
      <p><em>Action.</em> Audit SLIs quarterly. For each, ask: “If this gets worse, can a user notice? If users complain, does this move?” If not, demote to a diagnostic metric.</p>
    </section>

    <section class="section" id="10-2-next-steps">
      <h2>10.2 Next steps toward Advanced</h2>
      <p><strong>Why it matters.</strong> The jump from intermediate to advanced is not more jargon—it’s comfort with scale, failure, and cost under uncertainty. A deliberate learning path prevents thrash.</p>

      <h3 id="10-2-1-advanced-topics">10.2.1 Topics: capacity planning, multi‑region, cost optimization</h3>
      <p><strong>Plain.</strong> Advanced engineers make systems predictable before they are large, resilient before they break, and affordable before the bill surprises. <strong>Formal.</strong> Capacity planning estimates resource demand (CPU, memory, IO, network) under forecasted load with statistical safety margins; multi‑region architectures define <em>failure domains</em>, <em>RPO</em> (recovery point objective), and <em>RTO</em> (recovery time objective); cost optimization balances architecture choices against a monthly budget with <em>unit economics</em> (cost per user/request/message).</p>
      <ul>
        <li><strong>Capacity planning loop.</strong> Measure current headroom, model arrival patterns, project growth, and set <em>scaling guardrails</em> (autoscaler bounds, queue limits). Validate with load tests and track forecast error.</li>
        <li><strong>Multi‑region options.</strong> Active‑passive (failover), active‑active (global read/write), and regional isolation (cell‑based) each trade availability, consistency, and cost.</li>
        <li><strong>Cost optimization.</strong> Right‑size instances, prefer managed where developer time is scarce, move cold data to cheap storage, and set deletion/retention policies. Establish <em>cost SLOs</em> (e.g., telemetry budget, peak infra budget) to bound spend volatility (AWS Well‑Architected, 2025).</li>
      </ul>

      <p>Compare: <strong>Multi‑region strategies</strong></p>
      <table class="table">
        <thead><tr><th>Strategy</th><th>Data model</th><th>Pros</th><th>Cons</th><th>Use when</th></tr></thead>
        <tbody>
          <tr><td>Active‑passive</td><td>Primary + async replica</td><td>Simpler; cheaper</td><td>RPO &gt; 0; failover drills needed</td><td>Most business apps; tolerate minutes of RTO</td></tr>
          <tr><td>Active‑active (read local, write global)</td><td>Partitioned writes + global coordination</td><td>Low read latency; higher availability</td><td>Complex conflict handling; cost</td><td>Global read‑heavy apps; tolerate eventual consistency</td></tr>
          <tr><td>Cell‑based (regional isolation)</td><td>Fully independent cells</td><td>Fault isolation; blast radius control</td><td>Operational duplication</td><td>Large scale; strong isolation needs</td></tr>
        </tbody>
      </table>

      <h3 id="10-2-2-research-skills">10.2.2 Research skills: reading papers & evaluating technologies</h3>
      <p><strong>Plain.</strong> Advanced work requires forming your own opinion about tools and papers. <strong>Formal.</strong> Evaluate with <em>claims</em> (what it promises), <em>assumptions</em> (workload, hardware), <em>evidence</em> (benchmarks, failure modes), and <em>fit</em> (your constraints). <strong>Pitfall.</strong> Treating a paper as a product or a vendor blog as proof.</p>
      <ul>
        <li><strong>Paper workflow.</strong> Read abstract and evaluation sections first; write a 5‑sentence summary in your own words; list 3 assumptions that don’t match your world; design a small experiment to test one claim.</li>
        <li><strong>Tool evaluation.</strong> Create a <em>scorecard</em>: problem fit, integration cost, operability, community/docs, lock‑in risk, TCO. Run a 
          2‑week spike with success criteria and an exit plan.</li>
        <li><strong>Decision record.</strong> Capture your choice in an ADR with at least two rejected alternatives and the <em>evidence</em> that would change your mind.</li>
      </ul>
      <p><em>Analogy.</em> Treat new technologies like renting a car in a foreign city: start slow, stay on main roads, and know how to turn around before you explore alleys.</p>
    </section>

    <section class="section" id="10-3-advanced-topics">
      <h2>10.3 Optional advanced topics (pick 2–3)</h2>
      <p><strong>Why it matters.</strong> A taste of the next tier helps you choose a focused path without boiling the ocean. Each subsection sketches the problem space, key techniques, trade‑offs, and a safe entry ramp.</p>

      <h3 id="10-3-1-global-consistency">10.3.1 Global consistency & geo‑replication strategies</h3>
      <p><strong>Plain.</strong> Users are everywhere, networks fail, and latency across oceans is real. Global systems balance <em>freshness</em> and <em>availability</em> with <em>compliance</em> and <em>cost</em>. <strong>Formal.</strong> Designs range from leader‑based replication with <em>read‑my‑writes</em> routing and bounded staleness, to partitioned leaders (<em>multi‑leader</em>) with conflict resolution, to true global consensus for a narrow core (CP under partition) (Kleppmann, 2017; SRE corpus).</p>
      <ul>
        <li><strong>Choices:</strong> geo‑partition by tenant or key; keep writes local to reduce latency and replicate async; or centralize the write path and cache/read local.</li>
        <li><strong>Techniques:</strong> quorum writes, <em>R + W &gt; N</em>; <em>fencing tokens</em> with leases; <em>session consistency</em> for UX; <em>bounded staleness</em> with freshness SLIs and UI hints.</li>
        <li><strong>Compliance:</strong> data residency may force partitioning; design APIs that reveal where data lives and how it moves.</li>
      </ul>
      <p><strong>Trade‑offs.</strong> Strong global consistency raises latency and cost; eventual consistency requires reconciliation and user‑visible safeguards. A hybrid often wins: CP for money‑moving, AP for feeds and presence.</p>
      <p><strong>Entry ramp.</strong> Start with a second region in <em>read‑only</em> mode and an <em>active‑passive</em> database. Practice failover and back. Add <em>read‑my‑writes</em> sticky routing for critical flows before considering global writes.</p>

      <h3 id="10-3-2-advanced-streaming">10.3.2 Advanced streaming & stateful processing at scale</h3>
      <p><strong>Plain.</strong> Streaming systems turn unbounded event flows into timely, correct results. <strong>Formal.</strong> Key concepts are <em>event time</em> vs <em>processing time</em>, <em>watermarks</em> (progress indicators), <em>windowing</em> (tumbling, sliding, sessions), <em>exactly‑once</em> semantics (idempotent sinks + transactional writes), and <em>state backends</em> with checkpoints. <strong>Pitfall.</strong> Assuming in‑order events; ignoring late data; using unbounded joins without guards.</p>
      <ul>
        <li><strong>Event time first.</strong> Publish timestamps at the producer; use watermarks to control lateness and trigger results; handle late data with updates or correction streams.</li>
        <li><strong>State size.</strong> Keep per‑key state bounded; evict with TTLs and compaction; monitor checkpoint duration.</li>
        <li><strong>Delivery semantics.</strong> Most brokers are at‑least‑once; design idempotent consumers and dedupe by keys. Exactly‑once claims usually hide constraints; verify in your stack.</li>
      </ul>
      <p><strong>Trade‑offs.</strong> Lower latency increases compute and duplicate work; tighter correctness (exactly‑once) increases coordination and cost. Choose the point that meets your SLIs at the lowest TCO.</p>
      <p><strong>Entry ramp.</strong> Convert one batch job (daily aggregates) into near real time with a small window (e.g., 5‑minute tumbling), measure freshness and cost, then iterate.</p>

      <h3 id="10-3-3-platform-engineering">10.3.3 Platform engineering: building internal developer platforms</h3>
      <p><strong>Plain.</strong> A platform team productizes paved roads for delivery: templates, build/test/deploy, observability, and guardrails. <strong>Formal.</strong> An <em>Internal Developer Platform</em> (IDP) offers golden paths (well‑supported tech stacks) and self‑service abstractions (create a service, get CI/CD, SLO dashboards, and a runbook skeleton) with <em>SLOs</em> for the platform itself (provisioning time, deploy success, incident MTTR).</p>
      <ul>
        <li><strong>Scope wisely.</strong> Start with the top three friction points (e.g., “create service,” “get a dashboard,” “deploy safely”).</li>
        <li><strong>Product mindset.</strong> Treat developers as customers; collect NPS‑style feedback; publish a roadmap and <em>support SLOs</em>.</li>
        <li><strong>Guardrails over gates.</strong> Prefer linting, templates, and scorecards to manual approvals. The platform should speed teams up while keeping them safe.</li>
      </ul>
      <p><strong>Trade‑offs.</strong> Platforms reduce duplicate toil but can calcify if over‑standardized. Build modular components and allow controlled escapes for novel workloads.</p>
      <p><strong>Entry ramp.</strong> Ship a CLI or wizard that scaffolds a new service with: repo, CI pipeline, container template, health probes, tracing hooks, SLO dashboard, alert skeletons, and a <em>one‑click rollback</em> job.</p>
    </section>

    <section class="section" id="10-4-applied-exercises">
      <h2>10.4 Applied exercises</h2>
      <ol>
        <li><strong>Pitfall audit (45 min).</strong> List your top five operational pains. For each, map to a pitfall in 10.1 and write one action (merge services, add ownership, write a telemetry budget, or fix an SLO).</li>
        <li><strong>Capacity plan (60 min).</strong> Pick one bottleneck service. Forecast 3‑month growth, set autoscaler bounds, and plan a load test to validate headroom. Record forecast error after the test.</li>
        <li><strong>Multi‑region mini‑design (60 min).</strong> Draft an active‑passive plan with RPO/RTO and a failover runbook. Define what runs read‑only during failover and how you’ll reconcile on failback.</li>
        <li><strong>Streaming upgrade (60 min).</strong> Take one batch aggregation and convert it to a 5‑minute windowed streaming job. Define freshness SLI and late‑data policy.</li>
        <li><strong>Platform starter kit (45 min).</strong> Create a template or script that generates a new service with CI, health probes, tracing, and a basic dashboard. Include a documented rollback path.</li>
      </ol>
    </section>

    <section class="section" id="10-5-resources">
      <h2>Resources</h2>
      <ul>
        <li><strong>Google SRE Book &amp; Workbook</strong> — SLOs, error budgets, incident practices, and organizational patterns. <em>Why:</em> align reliability with delivery; avoid SLO anti‑patterns. <em>(Free/Paid)</em></li>
        <li><strong>AWS Well‑Architected Framework</strong> — lenses for reliability, performance efficiency, and cost. <em>Why:</em> structured trade‑off prompts for capacity and multi‑region choices. <em>(Free)</em></li>
        <li><strong>Kubernetes docs</strong> — orchestration fundamentals for probes, autoscaling, and policies. <em>Why:</em> practical deployment building blocks. <em>(Free)</em></li>
        <li><strong>Martin Fowler’s essays</strong> — microservices boundaries, testing strategies, and decomposition cautions. <em>Why:</em> avoid distributed monoliths and over‑splitting. <em>(Free)</em></li>
        <li><strong>Designing Data‑Intensive Applications</strong> (Kleppmann) — consistency, replication, and log‑centric architectures. <em>Why:</em> deep mental models for global and streaming topics. <em>(Paid)</em></li>
      </ul>
      <p>Where the above sources disagree, prefer <em>measured</em> claims (benchmarks, incident analyses) over anecdote. Document your local context before adopting guidance.</p>
    </section>

    <section class="section" id="10-6-recap-next">
      <h2>Recap &amp; Next Steps</h2>
      <ul>
        <li>You can spot and remediate three high‑leverage pitfalls: over‑splitting, ignoring operational cost/observability, and misaligned SLOs.</li>
        <li>You have a concrete ramp to advanced work: capacity planning with headroom, a pragmatic multi‑region strategy, and cost guardrails.</li>
        <li>You’ve sampled advanced areas—global consistency, streaming, and platform engineering—and have an entry plan for each if it fits your context.</li>
        <li>You know how to keep learning deliberately: summarize papers, run spikes with scorecards, and record ADRs with explicit “revisit when…” triggers.</li>
      </ul>
      <p><strong>Next:</strong> Move to the <a href="chapters/appendix.html#appendix-hero">Appendix</a> for a final coverage checklist, a progression map to the advanced tier, and a “top 10 pitfalls + fixes” quick‑reference you can print and bring to design reviews.</p>
    </section>

    <nav class="next-prev">
      <a class="btn" rel="prev" href="chapters/ch09.html">← Previous</a>
      <a class="btn btn-primary" rel="next" href="chapters/appendix.html">Next →</a>
    </nav>

    <footer class="site-footer">
      <div class="container">
        <p class="muted">© 2025 BookBuilder. Built with vanilla HTML/CSS/JS. Dark theme.</p>
      </div>
    </footer>
  </main>

  <!--
  CHECKLIST (Chapter 10)
  - [x] /styles/theme.css + /scripts/app.js linked; <base> correct; no inline nav JS
  - [x] Canonical nav (Home / Appendix / Glossary only)
  - [x] Pager prev → ch09, next → appendix; ToC numbering matches
  - [x] Order: Hero → Numbered Sections → Resources → Recap
  - [x] ≥1,800 words of prose (body text)
  - [x] Images: none (no audit required)
  - [x] Head/meta complete; no TODOs
  -->
</body>
</html>

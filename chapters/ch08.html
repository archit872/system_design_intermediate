<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Chapter 08 — Patterns, Trade-offs & Architecture Anti-Patterns</title>
  <meta name="description" content="Recognize and apply scalable patterns (CQRS, event sourcing, bulkheads, circuit breakers), select integration and consistency patterns (Sagas, materialized views, caching), and avoid anti-patterns (distributed monolith, premature microservices) while weighing cost-performance trade-offs.">
  <meta property="og:title" content="Chapter 08 — Patterns, Trade-offs & Architecture Anti-Patterns">
  <meta property="og:description" content="Practical patterns and pitfalls for real systems: when to use CQRS, event sourcing, bulkheads, throttling; how to run Sagas and read models; how to escape a distributed monolith; and how to manage cost/perf trade-offs.">
  <meta property="og:type" content="article">
  <base href="../">
  <link rel="stylesheet" href="styles/theme.css">
  <script src="scripts/app.js" defer></script>
</head>
<body>
  <a class="skip-link" href="#main">Skip to main content</a>

  <header class="app-nav">
    <div class="container inner">
      <div class="brand">Intermediate System Design</div>
      <button class="toggle js-nav-toggle" aria-expanded="false" aria-controls="top-menu">☰ Menu</button>
      <nav id="top-menu" class="menu" aria-label="Primary">
        <a href="index.html">Home</a>
        <a href="chapters/appendix.html">Appendix</a>
        <a href="chapters/glossary.html">Glossary</a>
      </nav>
    </div>
  </header>

  <main id="main" class="container fade-in">
    <section class="page-hero" id="8-hero">
      <div class="meta"><span class="badge badge-primary">Chapter 08</span></div>
      <h1>Patterns, Trade-offs &amp; Architecture Anti-Patterns</h1>
      <p class="abstract">Patterns are named solutions to recurring problems. They are useful <em>only</em> when you know the problem you have, the properties you need, and the costs you are willing to pay. This chapter equips you to deploy a small set of production-grade patterns—CQRS, event sourcing, bulkheads, circuit breakers, throttling, Sagas, and read models—while recognizing and refactoring anti-patterns like the distributed monolith and premature microservices. We close with cost-performance trade‑offs so you can justify choices in the language of both reliability and budget.</p>
    </section>

    <section class="section" id="8-1-common-patterns">
      <h2>8.1 Common scalable patterns</h2>
      <p><strong>Why it matters.</strong> Patterns reduce accidental complexity by encoding prior experience. But every pattern introduces new risks. Knowing <em>when not</em> to use a pattern is as important as knowing how it works.</p>

      <h3 id="8-1-1-cqrs-event-sourcing">8.1.1 CQRS and event sourcing: where they help and costs</h3>
      <p><strong>Plain.</strong> <em>CQRS</em> (Command Query Responsibility Segregation) means you separate the models and paths for writes (commands) and reads (queries). <em>Event sourcing</em> stores the sequence of domain events as the system of record and rebuilds read models from those events. <strong>Formal.</strong> In CQRS, the write model enforces invariants; read models are projections optimized for queries and may be eventually consistent. Event sourcing requires an append-only event log, deterministic projection functions, and versioned event schemas. <strong>Pitfall.</strong> Using event sourcing when you only need auditing—or using CQRS when a well-indexed relational schema and a couple of materialized views would suffice—adds cognitive load, on-call surface area, and migration complexity.</p>

      <p>Compare: <strong>When to use CQRS vs event sourcing</strong></p>
      <table class="table">
        <thead><tr><th>Need</th><th>CQRS</th><th>Event sourcing</th><th>Notes</th></tr></thead>
        <tbody>
          <tr><td>Low-latency reads tailored per UI</td><td>Excellent (custom read models)</td><td>Also good (via projections)</td><td>CQRS without ES keeps writes simple</td></tr>
          <tr><td>Full audit and replay</td><td>Possible but partial</td><td>Native strength</td><td>ES shines when history is a feature</td></tr>
          <tr><td>Simple CRUD app</td><td>Overkill</td><td>Overkill</td><td>Use RDBMS with indexes/materialized views</td></tr>
          <tr><td>Complex invariants on write</td><td>Good (focused domain model)</td><td>Good but harder (idempotent events)</td><td>Beware multi-aggregate transactions</td></tr>
        </tbody>
      </table>

      <p><strong>Worked example A — Orders with CQRS, no event sourcing.</strong> Write side: transactional RDBMS enforces stock reservation and payment invariants. Read side: <em>order_view</em> table denormalized for customer queries. Updates flow from write DB to the view via CDC. <em>Trade‑off:</em> simplicity on writes; eventual consistency on reads is OK because order summaries can lag by seconds.</p>
      <p><strong>Worked example B — Ledger with event sourcing.</strong> Each <code>credit</code> and <code>debit</code> is an immutable event. The authoritative balance is the sum of events. Read models project running balances per account and time window. <em>Trade‑offs:</em> expensive backfills if event schema changes; stronger tooling needed for idempotency and replay. <em>Benefit:</em> perfect audit and the ability to reconstruct balances at any point in time.</p>

      <h3 id="8-1-2-circuit-bulkhead-throttle">8.1.2 Circuit breakers, bulkheads, throttling</h3>
      <p><strong>Plain.</strong> <em>Circuit breakers</em> fail fast when a dependency is unhealthy. <em>Bulkheads</em> isolate resources so one noisy neighbor can’t sink the ship. <em>Throttling</em> limits the rate of work to protect capacity. <strong>Formal.</strong> A circuit moves between <em>closed</em> (pass), <em>open</em> (block), and <em>half‑open</em> (probe) based on error rate/latency windows; bulkheads allocate connection pools/queues per tenant or endpoint; throttling uses token buckets/leaky buckets to bound concurrency and rate. <strong>Pitfall.</strong> Breakers that trip on <em>client errors</em> (4xx) or misconfigured probe windows that flap; global connection pools that allow a surge from one route to starve others.</p>

      <p>Compare: <strong>Isolation and protection patterns</strong></p>
      <table class="table">
        <thead><tr><th>Pattern</th><th>Protects against</th><th>How</th><th>When to use</th><th>Gotchas</th></tr></thead>
        <tbody>
          <tr><td>Circuit breaker</td><td>Cascading retries</td><td>Open on failure/latency; half‑open probes</td><td>Remote dependencies</td><td>Tune windows; exempt idempotent 4xx</td></tr>
          <tr><td>Bulkhead</td><td>Noisy routes/tenants</td><td>Per‑route pools/queues</td><td>Multi‑tenant, mixed QPS</td><td>Requires capacity planning per pool</td></tr>
          <tr><td>Throttling</td><td>Overload from clients</td><td>Token bucket on ingress</td><td>APIs with spikes</td><td>Return Retry‑After; protect critical paths</td></tr>
        </tbody>
      </table>

      <pre data-lang="pseudo"><code>// Token bucket throttle (simplified)
if (bucket.tokens &gt; 0) {
  bucket.tokens--;
  handle();
} else {
  respond 429 with Retry-After
}
// Refill tokens every interval up to capacity
</code></pre>

      <p><strong>Analogy.</strong> Think of bulkheads like compartmentalized ship hulls: a breach floods one compartment, not the whole vessel. Circuit breakers are the watertight doors that slam shut when sensors detect trouble. Throttling is the harbor master limiting how many boats can enter at once.</p>
    </section>

    <section class="section" id="8-2-integration-consistency">
      <h2>8.2 Integration and data consistency patterns</h2>
      <p><strong>Why it matters.</strong> Distributed transactions across services are fragile and slow. Integration patterns provide <em>compositional</em> ways to coordinate state without global locks.</p>

      <h3 id="8-2-1-saga">8.2.1 Saga pattern for long‑running transactions</h3>
      <p><strong>Plain.</strong> A <em>Saga</em> splits a long transaction into a sequence of steps with <em>compensating</em> actions to undo partial work if later steps fail. <strong>Formal.</strong> Two styles: <em>orchestrated</em> (a central coordinator issues commands and awaits replies) and <em>choreographed</em> (services publish events and react). <strong>Pitfall.</strong> Partial failure handling is often an afterthought; Sagas require explicit compensation design and idempotency for every step.</p>

      <p>Compare: <strong>Orchestrated vs choreographed Sagas</strong></p>
      <table class="table">
        <thead><tr><th>Aspect</th><th>Orchestrated</th><th>Choreographed</th><th>Guidance</th></tr></thead>
        <tbody>
          <tr><td>Control flow</td><td>Central brain</td><td>Decentralized via events</td><td>Start orchestrated for clarity; move side‑effects to events</td></tr>
          <tr><td>Observability</td><td>Easier (single trace)</td><td>Harder (many services)</td><td>Propagate correlation IDs; add a saga log</td></tr>
          <tr><td>Coupling</td><td>Higher</td><td>Lower</td><td>Keep orchestration slim; avoid god‑service</td></tr>
          <tr><td>Failure handling</td><td>Centralized</td><td>Distributed</td><td>Document compensation per step either way</td></tr>
        </tbody>
      </table>

      <pre data-lang="json"><code>{
  "saga": "checkout",
  "steps": [
    { "try": "reserve_stock", "compensate": "release_stock" },
    { "try": "charge_card",   "compensate": "refund_charge" },
    { "try": "create_shipment", "compensate": "cancel_shipment" }
  ],
  "timeoutMs": 30000,
  "idempotencyKey": "order-123"
}</code></pre>

      <p><strong>Worked example C — Travel booking.</strong> Flights, hotel, and car must all succeed. Orchestrator performs steps in reliability order (most scarce first). If hotel fails, car is cancelled and flight is refunded. <em>Trade‑off:</em> stale holds and refunds cost money; add time‑boxed reservations and periodic reconciliation to clear stale holds.</p>

      <h3 id="8-2-2-materialized-views-caching">8.2.2 Materialized views and caching strategies</h3>
      <p><strong>Plain.</strong> Read models and caches make queries fast by precomputing or storing recent results. <strong>Formal.</strong> A materialized view is a precomputed table or index; a cache is a limited store with eviction. Both trade write amplification for read speed, and both risk serving stale data. <strong>Pitfall.</strong> Relying on TTL alone for coherence—bursts of invalidations can stampede origins; write‑back caches can lose data without durable journals.</p>

      <ul>
        <li><strong>Coherency:</strong> use <em>cache tags</em>/surrogate keys to invalidate related items (e.g., all products for a seller).</li>
        <li><strong>Stampede control:</strong> request coalescing; jittered TTLs; background refresh for hot keys.</li>
        <li><strong>Freshness SLIs:</strong> define max age by page/feature; expose last‑updated to the UI; revalidate at critical boundaries (e.g., checkout).</li>
        <li><strong>Backfill/rebuild:</strong> store the log (CDC/events) so views can be rebuilt after incidents.</li>
      </ul>

      <p><strong>Worked example D — Search facet counts.</strong> Precompute facet counts hourly into a view for fast filters; on index updates, send change events to update coarse aggregates more frequently. <em>Trade‑off:</em> counts may lag; mitigate by marking facets “≈” when staleness exceeds a threshold.</p>
    </section>

    <section class="section" id="8-3-anti-patterns">
      <h2>8.3 Anti‑patterns &amp; how to refactor them</h2>
      <p><strong>Why it matters.</strong> Many systems fail not because of a missing pattern but because of an unexamined anti‑pattern. Spotting the smell early saves months of thrash.</p>

      <h3 id="8-3-1-distributed-monolith">8.3.1 Distributed monolith and tight coupling</h3>
      <p><strong>Plain.</strong> A distributed monolith is a set of services that must deploy together, share databases, or chat synchronously for every user request. <strong>Formal.</strong> Operational coupling shows up as correlated deploys, cross‑service transactions, and shared tables. <strong>Pitfall.</strong> Extracting services by CRUD tables instead of cohesive capabilities; adding an API gateway stuffed with business logic that everyone must release with.</p>

      <p>Compare: <strong>Monolith → Healthy services (refactor path)</strong></p>
      <table class="table">
        <thead><tr><th>Smell</th><th>Why harmful</th><th>Refactor step</th><th>Success metric</th></tr></thead>
        <tbody>
          <tr><td>Shared DB writes</td><td>Breaks ownership</td><td>Introduce service‑owned write APIs; move non‑owners to read models</td><td>No non‑owner writes in audit logs</td></tr>
          <tr><td>Lock‑step deploys</td><td>Slow, fragile</td><td>Version‑tolerant contracts; CDC tests</td><td>Independent deploys without breakage</td></tr>
          <tr><td>Chatty calls on hot path</td><td>Latency & failure propagation</td><td>Aggregate in a gateway <em>adapter</em> or create a read model</td><td>≤ 2 service hops per user request</td></tr>
          <tr><td>Implicit invariants</td><td>Data races</td><td>Make invariants explicit in owning service; publish events</td><td>Invariants expressed as tests/alerts</td></tr>
        </tbody>
      </table>

      <p><strong>Refactor playbook.</strong> (1) Map dependencies and request flows; (2) pick one capability to isolate; (3) create a thin anti‑corruption layer; (4) route just that capability via gateway rules; (5) backfill read models; (6) deprecate shared table writes; (7) measure hop count and deploy independence.</p>

      <h3 id="8-3-2-premature-microservices">8.3.2 Over‑engineering: premature microservices</h3>
      <p><strong>Plain.</strong> Splitting into many services before the domain and team boundaries are clear increases overhead, slows delivery, and magnifies outages. <strong>Formal.</strong> Microservices add network, deployment, testing, and on‑call costs. <strong>Pitfall.</strong> Believing that “more services = more scale” without evidence; ignoring per‑service fixed costs (pipelines, dashboards, runbooks, on‑call).</p>

      <ul>
        <li><strong>Consolidation strategy:</strong> if two services always change together or chat synchronously, merge them. Keep the API boundary but collapse the deployable until evidence suggests splitting again.</li>
        <li><strong>Team fit:</strong> a service is healthy when a small team can own code + on‑call + data + roadmap. If ownership is split, either consolidate or redraw boundaries.</li>
        <li><strong>Measure debt:</strong> count flags, pipelines, dashboards, alerts. If counts scale with services but user value does not, you’re paying a microservice tax.</li>
      </ul>

      <p><strong>Worked example E — From 14 services to 5.</strong> A startup decomposed by nouns (user, profile, preferences, notifications…). Latency suffered; on‑call was noisy. The refactor merged UI‑coupled services into a single <em>Identity</em> service and collapsed low‑QPS side‑effects behind async jobs. Result: 40% latency reduction and 60% fewer incidents; future splits will follow change cadence, not nouns.</p>
    </section>

    <section class="section" id="8-4-cost-tradeoffs">
      <h2>8.4 Cost‑performance trade‑offs</h2>
      <p><strong>Why it matters.</strong> Architecture is economics with APIs. Costs—cloud, people, complexity—must be balanced against latency, availability, and delivery speed. Explicit trade‑offs prevent “boil the ocean” designs.</p>

      <h3 id="8-4-1-operational-cost-dev-speed">8.4.1 Operational cost vs development speed</h3>
      <p><strong>Plain.</strong> The cheapest runtime often increases developer time; the fastest developer path may waste compute. <strong>Formal.</strong> Minimize <em>total cost of ownership</em> (TCO): cloud + licenses + toil + incident cost + delay cost. <strong>Pitfall.</strong> Focusing on per‑request compute without noticing that on‑call toil or slow iteration dwarfs infra savings.</p>

      <p>Compare: <strong>Platform choice impact on TCO</strong></p>
      <table class="table">
        <thead><tr><th>Choice</th><th>Dev speed</th><th>Run cost</th><th>Ops load</th><th>Risk</th><th>Use when</th></tr></thead>
        <tbody>
          <tr><td>Managed serverless</td><td>High</td><td>Low at low QPS</td><td>Low</td><td>Latency jitter; limits</td><td>Spiky/seasonal or small apps</td></tr>
          <tr><td>Managed K8s</td><td>Medium</td><td>Medium</td><td>Medium</td><td>Cluster complexity</td><td>Steady multi‑service backends</td></tr>
          <tr><td>Self‑managed VMs</td><td>Low</td><td>Potentially low</td><td>High</td><td>Patching toil; drift</td><td>Custom kernels; legacy constraints</td></tr>
        </tbody>
      </table>

      <p><strong>Worked example F — Messaging service spend.</strong> You run 10k msgs/s steady, burst to 50k. Option 1: hosted broker with per‑message pricing; Option 2: self‑managed cluster on K8s. Calculate: hosted costs ₹X/month (per million messages), minimal ops; self‑managed cuts direct cost 30% but adds 20 hours/month SRE time and higher incident risk. If your team is feature‑constrained, the hosted option may win despite higher invoice.</p>

      <h3 id="8-4-2-observability-retention">8.4.2 Observability vs data retention costs</h3>
      <p><strong>Plain.</strong> Telemetry that explains production issues is invaluable—but storage and indexing are not free. <strong>Formal.</strong> Choose a <em>tiered retention</em> policy: hot (hours–days) for fast queries, warm (days–weeks) for recent hunts, cold (months) in cheap storage. <strong>Pitfall.</strong> Indexing every field for every log or storing full‑fidelity traces for months; costs explode silently.</p>

      <ul>
        <li><strong>Sampling:</strong> head‑based for steady savings; tail‑based to keep interesting/slow traces. Keep exemplars for linking metrics → traces.</li>
        <li><strong>Cardinality discipline:</strong> restrict labels on metrics; high‑cardinality details go into logs/traces.</li>
        <li><strong>Budget alerts:</strong> treat telemetry as a product with a budget; alert when ingestion or index size deviates from plan.</li>
      </ul>

      <p><strong>Worked example G — Log cost cap.</strong> A team capped daily log ingest at 150 GB. They adopted structured JSON, reduced noise by 60% with sampling and log levels, and pushed rare debug details to on‑demand trace events. Post‑incident analysis time dropped because the remaining logs were higher signal.</p>
    </section>

    <section class="section" id="8-5-applied-scenarios">
      <h2>8.5 Applied scenarios</h2>
      <p><strong>Scenario H — CQRS adoption checklist.</strong> Before adopting CQRS, answer: (1) Which queries are slow, and will a projection fix them? (2) What freshness SLI will we expose? (3) How will we rebuild projections after incidents? (4) What is the migration path to keep legacy endpoints working during the switch? If you cannot answer all four, start with materialized views, not a full CQRS split.</p>
      <p><strong>Scenario I — Circuit breaker tuning.</strong> A downstream has P95=120 ms, P99=300 ms. Start with breaker open when <em>error_rate ≥ 2%</em> or <em>latency &gt; 500 ms for 3 windows</em>; half‑open probes every 5 s with 1 request; close after 5 consecutive successes. Exclude 4xx from error rate; mark only idempotent methods as retryable.</p>
      <p><strong>Scenario J — Saga failure drill.</strong> In checkout, deliberately fail payment at 2% rate in staging: verify refund and stock release compensations occur, DLQ depth stays bounded, and user sees actionable status. Track time‑to‑consistency from failure to compensation.</p>
      <p><strong>Scenario K — Distributed monolith escape.</strong> Choose one invariant (“price at purchase time”). Move ownership to a single service; expose a <em>quote</em> API. Replace cross‑service DB writes with events. Success is measured by independent deploys and fewer cross‑service incidents.</p>
    </section>

    <section class="section" id="8-6-mini-exercises">
      <h2>8.6 Mini‑exercises</h2>
      <ol>
        <li><strong>Pattern fit matrix.</strong> For your capstone, create a matrix mapping each endpoint to patterns you plan to use (none/CQRS/materialized view/event sourcing/saga/bulkhead/breaker). Justify each choice in one sentence.</li>
        <li><strong>Circuit & bulkhead plan.</strong> Pick two high‑QPS routes. Define breaker thresholds, half‑open probes, and per‑route connection pools. Add <em>Retry‑After</em> and an error banner plan for the UI.</li>
        <li><strong>Saga compensation inventory.</strong> List try/compensate pairs for one long‑running flow (e.g., order → payment → shipment). Note idempotency keys and DLQ policy.</li>
        <li><strong>Cost guardrails.</strong> Draft a one‑page telemetry budget: retention tiers, sampling policy, and monthly GB limits with alerts. Include who approves changes.</li>
      </ol>
    </section>

    <section class="section" id="8-7-resources">
      <h2>Resources</h2>
      <ul>
        <li><strong>Architecture blogs (Fowler et al.)</strong> — pragmatic discussions of CQRS, Sagas, and anti‑patterns. <em>Why:</em> helps you choose patterns only when they fit. <em>(Free)</em></li>
        <li><strong>AWS Well‑Architected</strong> — decision lenses for reliability and cost. <em>Why:</em> frames trade‑offs and risk. <em>(Free)</em></li>
        <li><strong>Google SRE resources</strong> — error budgets and operational techniques that interact with patterns (circuit breakers, bulkheads). <em>Why:</em> align patterns with SLOs. <em>(Free)</em></li>
        <li><strong>Designing Data‑Intensive Applications</strong> — deep dives on events, logs, and read models. <em>Why:</em> rigorous background for CQRS/ES decisions. <em>(Paid)</em></li>
      </ul>
    </section>

    <section class="section" id="8-8-recap-next">
      <h2>Recap &amp; Next Steps</h2>
      <ul>
        <li>You can now deploy patterns deliberately—CQRS, event sourcing, Sagas, bulkheads, breakers, throttling—knowing exactly which user promises they uphold and which costs they incur.</li>
        <li>You can recognize a distributed monolith early and refactor using ownership, projections, and contract evolution rather than adding more services.</li>
        <li>You can articulate cost‑performance trade‑offs and set telemetry budgets that keep observability sustainable.</li>
      </ul>
      <p><strong>Next:</strong> Head to <a href="chapters/ch09.html#9-hero">Chapter 9 — Capstone Project — Full System Design &amp; Implementation</a>, where you will apply the patterns to design, implement, and evaluate a production‑grade prototype.</p>
    </section>

    <nav class="next-prev">
      <a class="btn" rel="prev" href="chapters/ch07.html">← Previous</a>
      <a class="btn btn-primary" rel="next" href="chapters/ch09.html">Next →</a>
    </nav>

    <footer class="site-footer">
      <div class="container">
        <p class="muted">© 2025 BookBuilder. Built with vanilla HTML/CSS/JS. Dark theme.</p>
      </div>
    </footer>
  </main>

  <!--
  CHECKLIST (Chapter 08)
  - [x] /styles/theme.css + /scripts/app.js linked; <base> correct; no inline nav JS
  - [x] Canonical nav (Home / Appendix / Glossary only)
  - [x] Pager prev → ch07, next → ch09; ToC numbering matches
  - [x] Order: Hero → Numbered Sections → Resources → Recap
  - [x] ≥1,800 words of prose (body text)
  - [x] Images: none (no audit required)
  - [x] Head/meta complete; no TODOs
  -->
</body>
</html>

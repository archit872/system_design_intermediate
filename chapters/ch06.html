<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Chapter 06 — Consistency, Partitioning & Concurrency</title>
  <meta name="description" content="Explain consistency models, choose patterns that match application semantics, design for concurrency control and conflict resolution in distributed data, and handle partition edge cases with quorum strategies.">
  <meta property="og:title" content="Chapter 06 — Consistency, Partitioning & Concurrency">
  <meta property="og:description" content="Strong vs eventual consistency, CAP in practice, optimistic vs pessimistic locking, MVCC, CRDTs and reconciliation pipelines, and surviving partitions with quorum tactics.">
  <meta property="og:type" content="article">
  <base href="../">
  <link rel="stylesheet" href="styles/theme.css">
  <script src="scripts/app.js" defer></script>
</head>
<body>
  <a class="skip-link" href="#main">Skip to main content</a>

  <header class="app-nav">
    <div class="container inner">
      <div class="brand">Intermediate System Design</div>
      <button class="toggle js-nav-toggle" aria-expanded="false" aria-controls="top-menu">☰ Menu</button>
      <nav id="top-menu" class="menu" aria-label="Primary">
        <a href="index.html">Home</a>
        <a href="chapters/appendix.html">Appendix</a>
        <a href="chapters/glossary.html">Glossary</a>
      </nav>
    </div>
  </header>

  <main id="main" class="container fade-in">
    <section class="page-hero" id="6-hero">
      <div class="meta"><span class="badge badge-primary">Chapter 06</span></div>
      <h1>Consistency, Partitioning & Concurrency</h1>
      <p class="abstract">Correctness in distributed systems is a moving target shaped by time, failure, and competing writes. In this chapter you will learn how to select the right consistency model for your user experience, design concurrency control that matches your workload, and handle conflicts—either by avoiding them with sound boundaries or by resolving them deterministically. We close with practical guidance for surviving partitions (split‑brain) using quorum reads/writes, fencing tokens, and fallbacks that keep user trust intact.</p>
    </section>

    <section class="section" id="6-1-consistency-models">
      <h2>6.1 Consistency models</h2>
      <p><strong>Why it matters.</strong> Users experience <em>consistency</em> as “did my change stick and do others see the same world I do?” Picking a stricter model than needed wastes latency and availability; picking a weaker model without UX guardrails creates confusion and data loss.</p>

      <h3 id="6-1-1-strong-eventual">6.1.1 Strong vs eventual consistency and user‑facing trade‑offs</h3>
      <p><strong>Plain.</strong> <em>Strong consistency</em> means that once a write is acknowledged, every subsequent read sees it. <em>Eventual consistency</em> means replicas converge over time; readers may see older values temporarily. <strong>Formal.</strong> A system is linearizable if operations appear to happen atomically in real time; eventual consistency offers only convergence, not real‑time ordering. <strong>Pitfall.</strong> Teams often toggle consistency without changing <em>UX</em>. If you allow stale reads, surface staleness or use write‑through caches for critical fields.</p>

      <p>Compare: <strong>Choosing a consistency model</strong></p>
      <table class="table">
        <thead><tr><th>Use case</th><th>Required user property</th><th>Recommended model</th><th>Why</th><th>UX safeguards</th></tr></thead>
        <tbody>
          <tr><td>Payments & order finalization</td><td>Authoritative truth now</td><td>Strong / linearizable</td><td>Prevent double‑spend, legal record</td><td>Block until committed; display authoritative status</td></tr>
          <tr><td>Product catalog price display</td><td>Close enough; bounded staleness</td><td>Eventual with freshness SLI</td><td>Cheaper/faster at scale</td><td>Show last‑updated; revalidate on checkout</td></tr>
          <tr><td>Chat typing indicator</td><td>Timeliness over perfection</td><td>Eventual</td><td>Drop events; tolerate loss</td><td>Auto‑expire; no persistence guarantee</td></tr>
          <tr><td>User profile edits</td><td>Read your writes</td><td>Session (read‑my‑writes)</td><td>Only the editor must see change immediately</td><td>Sticky reads; invalidate client cache</td></tr>
        </tbody>
      </table>

      <p><strong>Worked example — Read‑my‑writes via routing.</strong> After updating a profile, send the user’s subsequent reads to the same leader or to a replica that has caught up. Implementation options: (a) a short‑lived <em>session affinity</em> cookie; (b) include the last write’s log index in the read request and wait for replicas to apply up to that index before serving.</p>

      <h3 id="6-1-2-cap">6.1.2 CAP theorem & practical implications</h3>
      <p><strong>Plain.</strong> In the presence of a network partition, a system must choose between full consistency (every node agrees) and full availability (every request gets a response). <strong>Formal.</strong> CAP frames behavior only <em>during partitions</em>. Outside partitions, you can often have both. <strong>Pitfall.</strong> Treating CAP as a menu (“pick any two”) obscures design space like <em>bounded staleness</em> and <em>sticky clients</em>.</p>
      <ul>
        <li><strong>AP‑leaning designs</strong> (available under partition) favor local writes and reconcile later. Use for feeds, counters, presence. Require conflict handling.</li>
        <li><strong>CP‑leaning designs</strong> (consistent under partition) choose unavailability for conflicting operations. Use for money movement or invariants that must not break.</li>
        <li><strong>Hybrid:</strong> many systems are CP for a narrow core (placing an order) and AP for surrounding features (recommendations, activity).</li>
      </ul>
      <p><em>Takeaway:</em> Explicitly mark which paths are CP and which are AP, and design UX/fallbacks accordingly.</p>
    </section>

    <section class="section" id="6-2-concurrency-control">
      <h2>6.2 Concurrency control</h2>
      <p><strong>Why it matters.</strong> Concurrent writes compete for truth. The wrong approach yields lost updates, deadlocks, or unacceptable latency. The right approach depends on contention level and the cost of “try again.”</p>

      <h3 id="6-2-1-optimistic-pessimistic">6.2.1 Optimistic vs pessimistic locking</h3>
      <p><strong>Plain.</strong> <em>Optimistic</em> concurrency assumes conflicts are rare: write with a version and reject if the version changed. <em>Pessimistic</em> concurrency uses locks to serialize access. <strong>Formal.</strong> Optimistic = <em>compare‑and‑set</em> (CAS) with retries; Pessimistic = <em>two‑phase locking</em> (2PL) or advisory locks. <strong>Pitfall.</strong> Optimistic updates on hot rows can starve progress; pessimistic locks can deadlock or destroy throughput if held too long.</p>

      <p>Compare: <strong>Optimistic vs pessimistic control</strong></p>
      <table class="table">
        <thead><tr><th>Dimension</th><th>Optimistic</th><th>Pessimistic</th><th>Guidance</th></tr></thead>
        <tbody>
          <tr><td>Contention</td><td>Low</td><td>High</td><td>Switch strategy as hotspots emerge</td></tr>
          <tr><td>Latency</td><td>Low average; retries on conflict</td><td>Higher; waits on lock</td><td>Budget for tail retries vs blocking</td></tr>
          <tr><td>Complexity</td><td>App‑level version checks</td><td>DB locks / lease managers</td><td>Prefer simple unless proven necessary</td></tr>
          <tr><td>Failure modes</td><td>Starvation, livelock</td><td>Deadlock, convoy effects</td><td>Detect & mitigate with timeouts</td></tr>
        </tbody>
      </table>

      <pre data-lang="http"><code>PUT /cart/123
If-Match: "v7"   # optimistic ETag precondition
{
  "add": { "sku": "S-42", "qty": 1 }
}
-->
409 Conflict  // version mismatch; client refetches and retries</code></pre>

      <p><strong>Worked example — Pessimistic lease with fencing token.</strong> For a single‑writer job (e.g., nightly aggregation), obtain a <em>monotonic lease</em> from a coordinator that issues <em>fencing tokens</em> (1, 2, 3, …). Downstream writes include the token; stores reject writes with tokens lower than the last accepted value. If a slow old worker resumes, its token is stale and its writes are ignored.</p>

      <h3 id="6-2-2-mvcc">6.2.2 Multi‑version concurrency control (MVCC) basics</h3>
      <p><strong>Plain.</strong> MVCC keeps multiple versions of a row so readers don’t block writers. Each transaction sees a snapshot. <strong>Formal.</strong> At <em>snapshot isolation</em>, reads see a consistent snapshot and writes conflict only if two concurrent transactions update the same row. <strong>Pitfall.</strong> Long‑running transactions bloat version storage and increase conflict probability; “phantoms” (new rows matching a predicate) appear unless you use predicate locks or serializable isolation.</p>

      <p>Compare: <strong>Isolation levels (typical RDBMS)</strong></p>
      <table class="table">
        <thead><tr><th>Level</th><th>Prevents</th><th>Allows</th><th>Use when</th></tr></thead>
        <tbody>
          <tr><td>Read committed</td><td>Dirty reads</td><td>Non‑repeatable reads, phantoms</td><td>Low contention, short transactions</td></tr>
          <tr><td>Repeatable read</td><td>Dirty & non‑repeatable reads</td><td>Phantoms (DB‑specific)</td><td>Stable views with some concurrency</td></tr>
          <tr><td>Serializable</td><td>All anomalies</td><td>—</td><td>Critical invariants, low concurrency</td></tr>
        </tbody>
      </table>

      <pre data-lang="sql"><code>-- Example: optimistic update with a version column
UPDATE inventory
   SET qty = qty - 1, version = version + 1
 WHERE sku = 'S-42' AND version = 7;
-- Check affected rows; if 0, someone else updated first → retry with new version</code></pre>

      <p><em>Takeaway:</em> Use optimistic control for user‑scoped records and UI‑driven retries; consider pessimistic locks or queues for high‑contention resources (counters, scarce inventory).</p>
    </section>

    <section class="section" id="6-3-conflict-reconciliation">
      <h2>6.3 Conflict resolution & reconciliation</h2>
      <p><strong>Why it matters.</strong> In partitioned or offline‑capable systems, concurrent updates arrive out of order. You must either design <em>conflict‑free</em> data types or build reconciliation pipelines that restore a coherent state.</p>

      <h3 id="6-3-1-crdts">6.3.1 CRDTs and conflict‑free designs</h3>
      <p><strong>Plain.</strong> A <em>CRDT</em> (Conflict‑free Replicated Data Type) is a data structure that can be merged without coordination and still converge. <strong>Formal.</strong> The merge function is associative, commutative, and idempotent; replicas that process the same set of updates (in any order) reach the same state. <strong>Pitfall.</strong> CRDTs don’t solve <em>all</em> problems; they’re ideal for counters, sets, maps, and grow‑only registers—not for transactions demanding invariants across objects.</p>

      <p>Compare: <strong>Common CRDTs</strong></p>
      <table class="table">
        <thead><tr><th>CRDT</th><th>What it models</th><th>How merge works</th><th>When to use</th></tr></thead>
        <tbody>
          <tr><td>G‑Counter</td><td>Monotonic counter</td><td>Per‑replica counts summed</td><td>Likes/favorites, coarse metrics</td></tr>
          <tr><td>PN‑Counter</td><td>Increment & decrement</td><td>Two G‑Counters (pos/neg)</td><td>Inventory deltas (non‑authoritative)</td></tr>
          <tr><td>G‑Set / 2P‑Set</td><td>Add‑only / add‑then‑remove</td><td>Union / adds minus removes</td><td>Tagging, membership lists</td></tr>
          <tr><td>LWW‑Register</td><td>Last write wins</td><td>Max by timestamp/vector</td><td>Profile fields where recency wins</td></tr>
          <tr><td>OR‑Set</td><td>Set with remove by tag</td><td>Unique IDs for adds; remove by ID</td><td>Collaborative lists with deletions</td></tr>
        </tbody>
      </table>

      <pre data-lang="pseudo"><code>// G-Counter sketch
struct GCounter { map&lt;replicaId,int64&gt; counts }
function increment(id): counts[id]++
function value(): return sum(counts.values())
function merge(a, b):
  for each key in union(keys(a), keys(b)):
    a.counts[key] = max(a.counts[key], b.counts[key])  // idempotent
  return a</code></pre>

      <p><strong>Analogy.</strong> CRDTs are like travel expense envelopes for each teammate: everyone adds receipts independently; when you merge envelopes, you take the maximum receipt count per person so duplicates don’t inflate totals.</p>

      <h3 id="6-3-2-reconciliation-pipelines">6.3.2 Reconciliation pipelines and compensating transactions</h3>
      <p><strong>Plain.</strong> When you cannot avoid conflicts, reconcile them: detect divergence, compute the winner, and apply compensations for losers. <strong>Formal.</strong> A reconciliation pipeline pulls from change logs, groups updates per entity, resolves by a policy (timestamp, vector clock, domain rule), and writes back a canonical state while emitting <em>compensating events</em> for downstreams. <strong>Pitfall.</strong> Silent conflicts are the worst; always measure <em>conflict rate</em> and <em>time‑to‑convergence</em>.</p>

      <ul>
        <li><strong>Detect:</strong> compare per‑entity versions or vector clocks across replicas; flag gaps.</li>
        <li><strong>Resolve:</strong> apply policy: LWW, max, domain rule (e.g., “highest bid wins”). For critical paths, prefer <em>user choice</em> (“we saw two edits—pick one”).</li>
        <li><strong>Compensate:</strong> emit events to undo/adjust dependent actions (refunds, restocking) and update read models.</li>
        <li><strong>Prove:</strong> store audit trails: who wrote, from where, and what policy chose the winner.</li>
      </ul>

      <p><strong>Worked example — Offline profile edits.</strong> Mobile clients edit a profile offline. Two devices change the display name. Policy: LWW by server time. Reconciler chooses the later timestamp and emits <code>profile_reconciled</code> with the losing value; UI shows a non‑blocking toast: “We updated your name; tap to review previous version.”</p>

      <p><em>Takeaway:</em> If conflicts are expected, make them safe, visible, and reversible.</p>
    </section>

    <section class="section" id="6-4-partitions">
      <h2>6.4 Partitioning edge cases</h2>
      <p><strong>Why it matters.</strong> Network partitions and clock skews are rare but inevitable at scale. Your design should constrain damage when replicas disagree or clocks drift.</p>

      <h3 id="6-4-1-partitions-quorums">6.4.1 Network partitions, split‑brain, and quorum strategies</h3>
      <p><strong>Plain.</strong> A <em>quorum</em> is a majority subset of replicas whose agreement is enough to proceed. <strong>Formal.</strong> In an <em>N</em>‑replica system, choose read quorum <em>R</em> and write quorum <em>W</em> such that <em>R + W &gt; N</em> to ensure overlap; then a read always sees at least one replica that participated in the latest write. <strong>Pitfall.</strong> Choosing <em>W = 1</em> for low latency without thinking about stale reads; choosing <em>R = 1, W = 1</em> makes the system AP with potentially divergent replicas.</p>

      <p>Compare: <strong>Quorum configurations</strong></p>
      <table class="table">
        <thead><tr><th>N (replicas)</th><th>R</th><th>W</th><th>Behavior</th><th>Use when</th></tr></thead>
        <tbody>
          <tr><td>3</td><td>2</td><td>2</td><td>CP‑leaning; higher latency</td><td>Critical configs, money</td></tr>
        <tr><td>3</td><td>1</td><td>2</td><td>Fresh writes; fast reads may be stale</td><td>Write‑heavy with tolerant reads</td></tr>
        <tr><td>5</td><td>3</td><td>3</td><td>CP‑leaning; survives 2 failures</td><td>Core metadata</td></tr>
          <tr><td>5</td><td>2</td><td>3</td><td>Faster reads; still overlap</td><td>Mixed workloads</td></tr>
        </tbody>
      </table>

      <p><strong>Split‑brain protection.</strong> Use <em>fencing tokens</em> with leases; prefer leader election with majority quorum; on minority partitions, <em>degrade</em> gracefully (read‑only mode) rather than attempt writes. For caches or session stores, tolerate divergence but add fast rejoin procedures and staleness caps.</p>

      <p><strong>Clock discipline.</strong> Use NTP/chrony and design for clock skew tolerance: avoid LWW by wall‑clock where correctness matters; prefer logical clocks (Lamport, vector) or server‑assigned timestamps.</p>

      <h3 id="6-4-2-client-fallbacks">6.4.2 Client fallbacks and UX under uncertainty</h3>
      <p><strong>Plain.</strong> In partial failures, the client can help. <strong>Formal.</strong> Provide <em>idempotency keys</em>, <em>retry‑after</em> hints, and <em>circuit‑breaker feedback</em> so the UI degrades predictably. <strong>Pitfall.</strong> Silent retries amplify load and duplicate writes.</p>
      <ul>
        <li><strong>Write buffering at the edge:</strong> when the origin is down, buffer idempotent writes client‑side with a visible “syncing” state.</li>
        <li><strong>Explicit offline modes:</strong> design small subsets of features to work offline with CRDTs or queued commands.</li>
        <li><strong>Conflict prompts:</strong> when user‑visible conflicts occur, prefer guided resolution (“keep mine”, “use server”, “merge”).</li>
      </ul>

      <p><em>Takeaway:</em> Reliability is a full‑stack property: backends define semantics; clients communicate uncertainty.
      </p>
    </section>

    <section class="section" id="6-5-applied-scenarios">
      <h2>6.5 Applied scenarios</h2>
      <p><strong>Scenario A — Presence service at scale (AP core, CP edges).</strong> Presence updates are frequent and lossy; model online/offline as a CRDT (LWW register with conservative expiry). For correctness at boundaries (e.g., “can only send message if recipient allows DMs”), maintain a CP gate backed by a strongly consistent store. <em>Lesson:</em> split requirements by consistency need.</p>
      <p><strong>Scenario B — Inventory reservation (CP with queues).</strong> Scarce inventory can’t oversell. Accept requests into a queue; a single reservation service updates stock under a pessimistic lock or serializable transaction; return a reservation token with expiry. <em>Lesson:</em> serialize scarce resources; let everything else be parallel.</p>
      <p><strong>Scenario C — Metrics pipeline (AP with reconciliation).</strong> Producers write to local partitions during cross‑region outages; consumers read with <em>R=1</em>. On heal, a reconciler merges partitions by offset; duplicates removed via idempotent keys. <em>Lesson:</em> availability first, then converge.</p>
      <p><strong>Scenario D — Collaborative document (CRDT).</strong> Use an operation‑based CRDT (e.g., OR‑Set for sections + LWW for formatting). On reconnect, clients merge operations; conflicts produce deterministic results; the UI highlights recent merges. <em>Lesson:</em> choose data types that make conflicts harmless.</p>
    </section>

    <section class="section" id="6-6-mini-exercises">
      <h2>6.6 Mini‑exercises</h2>
      <ol>
        <li><strong>Label CP vs AP:</strong> For your capstone, mark which user journeys must be CP (reject under partition) and which can be AP (accept & reconcile). Write the UX behavior for each.</li>
        <li><strong>Pick a concurrency strategy:</strong> Identify a hot record (e.g., counter, scarce SKU). Decide optimistic vs pessimistic and justify with expected contention and retry cost.</li>
        <li><strong>Design a reconciler:</strong> Sketch a pipeline that detects and resolves conflicts for one entity. Define the merge policy, metrics (conflict rate, time‑to‑converge), and compensations.</li>
        <li><strong>Quorum tuning:</strong> Choose N, R, W for one store; simulate two failure cases and explain read/write behavior and staleness exposure.</li>
      </ol>
    </section>

    <section class="section" id="6-7-resources">
      <h2>Resources</h2>
      <ul>
        <li><strong>Designing Data‑Intensive Applications</strong> — comprehensive coverage of consistency, replication, and transactions. <em>Why:</em> rigorous mental models for CAP, MVCC, and CRDT‑like patterns. <em>(Paid)</em></li>
        <li><strong>Microservices essays</strong> — practical guidance on data ownership and integration contracts that reduce cross‑service transactions. <em>Why:</em> boundary hygiene reduces conflicts. <em>(Free)</em></li>
        <li><strong>SRE sources</strong> — incident patterns around split‑brain, fencing, and error budgets during partitions. <em>Why:</em> operational tactics when theory meets outages. <em>(Free)</em></li>
      </ul>
    </section>

    <section class="section" id="6-8-recap-next">
      <h2>Recap & Next Steps</h2>
      <ul>
        <li>You can choose consistency deliberately (strong, eventual, session) and design UX to communicate freshness and uncertainty.</li>
        <li>You can select concurrency control (optimistic, pessimistic, MVCC) based on contention and retry costs, and implement fences for single writers.</li>
        <li>You can avoid conflicts with CRDTs where appropriate, or reconcile deterministically with auditable pipelines and compensations.</li>
        <li>You can plan quorum configurations and fallbacks that constrain damage during partitions and guide clients safely.</li>
      </ul>
      <p><strong>Next:</strong> Continue to <a href="chapters/ch07.html#7-hero">Chapter 7 — Operational Practices: CI/CD, Observability &amp; Security</a>, where we connect releases, telemetry, and security into a sustainable delivery system.</p>
    </section>

    <nav class="next-prev">
      <a class="btn" rel="prev" href="chapters/ch05.html">← Previous</a>
      <a class="btn btn-primary" rel="next" href="chapters/ch07.html">Next →</a>
    </nav>

    <footer class="site-footer">
      <div class="container">
        <p class="muted">© 2025 BookBuilder. Built with vanilla HTML/CSS/JS. Dark theme.</p>
      </div>
    </footer>
  </main>

  <!--
  CHECKLIST (Chapter 06)
  - [x] /styles/theme.css + /scripts/app.js linked; <base> correct; no inline nav JS
  - [x] Canonical nav (Home / Appendix / Glossary only)
  - [x] Pager prev → ch05, next → ch07; ToC numbering matches
  - [x] Order: Hero → Numbered Sections → Resources → Recap
  - [x] ≥1,800 words of prose (body text)
  - [x] Images: none in this chapter (no audit required)
  - [x] Head/meta complete; no TODOs
  -->
</body>
</html>

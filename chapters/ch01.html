<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Chapter 01 — Introduction & How this Builds on Beginner End Goals</title>
  <meta name="description" content="Chapter 1 sets expectations, connects to beginner outcomes, defines milestones and evaluation, and helps you pick a capstone domain with baseline NFRs and SLOs.">
  <meta property="og:title" content="Chapter 01 — Introduction & How this Builds on Beginner End Goals">
  <meta property="og:description" content="From vocabulary to production: pick a capstone, define NFRs and SLOs, and set up a 12–16 week roadmap with reviews, dashboards, and postmortems.">
  <meta property="og:type" content="article">
  <base href="../">
  <link rel="stylesheet" href="styles/theme.css">
  <script src="scripts/app.js" defer></script>
</head>
<body>
  <a class="skip-link" href="#main">Skip to main content</a>

  <header class="app-nav">
    <div class="container inner">
      <div class="brand">Intermediate System Design</div>
      <button class="toggle js-nav-toggle" aria-expanded="false" aria-controls="top-menu">☰ Menu</button>
      <nav id="top-menu" class="menu" aria-label="Primary">
        <a href="index.html">Home</a>
        <a href="chapters/appendix.html">Appendix</a>
        <a href="chapters/glossary.html">Glossary</a>
      </nav>
    </div>
  </header>

  <main id="main" class="container fade-in">
    <section class="page-hero" id="1-hero">
      <div class="meta"><span class="badge badge-primary">Chapter 01</span></div>
      <h1>Introduction & How this Builds on Beginner End Goals</h1>
      <p class="abstract">Beginner‑level system design gives you names for parts and a way to sketch a single request path. This chapter turns that vocabulary into a plan for building production‑grade systems. You will choose a capstone domain, translate features into non‑functional requirements (NFRs), set concrete SLO/SLI targets, and adopt a realistic 12–16 week cadence with design reviews, dashboards, and a final postmortem.</p>
    </section>

    <figure>
      <img src="https://www.oreilly.com/covers/urn%3Aorm%3Abook%3A9781491903063/400w/" alt="Cover of Designing Data‑Intensive Applications by Martin Kleppmann" loading="lazy" decoding="async" width="400" height="525" referrerpolicy="no-referrer">
      <figcaption>Core reference: <em>Designing Data‑Intensive Applications</em> (O'Reilly Media, 2017) — for data trade‑offs and consistency models.</figcaption>
    </figure>

    <section class="section" id="1-1-course-roadmap">
      <h2>1.1 Course roadmap & timeframe</h2>
      <p><strong>Why it matters.</strong> Without a shared pace and checkpoints, teams drift toward feature‑first delivery and postpone operational work. A roadmap aligns design, implementation, and reliability milestones so that every architectural choice is exercised under load and observed in production‑like environments (Google SRE, 2016; AWS Well‑Architected, 2024).</p>
      <p><strong>Plain.</strong> We will spend about three months moving from concepts to a working prototype that you can deploy, measure, and break safely. <strong>Formal.</strong> The program is a staged plan with deliverables (NFRs, SLOs, runbooks, dashboards, and a postmortem) and acceptance criteria tied to a rubric. <strong>Pitfall.</strong> Many learners leave SLOs and rollback plans until the end; this chapter makes them first‑class milestones. <strong>Example.</strong> In week 2 you’ll pick a data store with written trade‑offs; by week 6 your CI/CD pipeline will ship a service behind a feature flag with an automated rollback.</p>

      <h3 id="1-1-1-timeline">1.1.1 Timeline & weekly cadence (12–16 weeks)</h3>
      <ul>
        <li><strong>Week 0:</strong> Choose a capstone domain; draft NFRs (<em>availability, latency, cost, privacy</em>) and initial SLIs/SLOs.</li>
        <li><strong>Weeks 1–3:</strong> Component design, data model, API contracts; write two alternatives for any key decision (e.g., "RDBMS vs document store").</li>
        <li><strong>Weeks 3–6:</strong> Implement core services; stand up CI/CD with basic tests; enable traces and metrics; publish a design log.</li>
        <li><strong>Weeks 6–9:</strong> Deploy to a managed runtime (Kubernetes/serverless); add dashboards and two alerting rules; run a baseline load test.</li>
        <li><strong>Weeks 9–12:</strong> Fault injection experiment; blameless postmortem; polish docs and demo.</li>
      </ul>
      <p><strong>Trade‑offs.</strong> A shorter 8–10 week path increases focus but reduces time for iteration and refactoring; a 16‑week path allows deeper experiments (e.g., alternative messaging backbones) but requires firmer scope control.</p>

      <h3 id="1-1-2-checkpoints">1.1.2 Checkpoints and milestone deliverables</h3>
      <p><strong>Plain.</strong> Checkpoints are safety rails. <strong>Formal.</strong> Each checkpoint has artifacts that demonstrate learning outcomes and de‑risk later stages. <strong>Pitfall.</strong> Skipping a checkpoint often produces surprises during load tests (e.g., unbounded fan‑out or hot partitions). <strong>Example.</strong> A “definition of done” for Milestone 2 is “CI/CD deploys to a staging environment, feature flags are enabled, and error rates are visible on a dashboard.”</p>
      <ol>
        <li><strong>Checkpoint A — NFRs & SLOs drafted.</strong> Target users, expected concurrency, 95th percentile latency budgets, and an <em>error budget</em> (Google SRE, 2016).</li>
        <li><strong>Checkpoint B — Architecture doc & data model.</strong> Sequence diagrams plus read/write paths; documented backpressure and caching strategies.</li>
        <li><strong>Checkpoint C — CI/CD & Observability baseline.</strong> Automated tests, deployment strategy (blue/green or canary), metrics/logs/traces (OpenTelemetry‑style), and two alert rules.</li>
        <li><strong>Checkpoint D — Fault Injection & Postmortem.</strong> One chaos experiment (e.g., dependency latency spike) and a blameless postmortem with corrective actions prioritized.</li>
      </ol>
      <p><em>Takeaway:</em> Checkpoints force you to prove reliability early instead of hoping it emerges at the end.</p>
      <h3 id="1-1-3-scope">1.1.3 Timeboxing & scope control</h3>
      <p><strong>Plain.</strong> The fastest way to fail a system design capstone is to keep adding scope. <strong>Formal.</strong> Define a <em>Minimum Viable Slice (MVS)</em>: a thin vertical feature that crosses UI → API → storage → observability → rollback. Ship the MVS by week 4 and iterate. <strong>Pitfall.</strong> Horizontal slices ("build the whole storage layer first") defer end‑to‑end learning and hide coupling issues until late. <strong>Example.</strong> For chat, choose one conversation view with five users, not the entire workspace model; for metrics, ship one event type with one consumer.</p>
      <ul>
        <li><strong>Timebox unknowns:</strong> Allocate 1–2 days to spike alternatives (e.g., gRPC vs REST) and decide with an ADR. Revisit only if measurements break assumptions.</li>
        <li><strong>Budget iteration:</strong> Reserve 20–30% of each week for refactoring, documentation, and stabilizing tests.</li>
        <li><strong>Exit criteria:</strong> Each timebox ends with a yes/no decision and a short note on trade‑offs and risks carried forward.</li>
      </ul>
      <p><em>Takeaway:</em> Ship a narrow end‑to‑end slice early; prefer decisions with a trial plan over indefinite research.</p>
    </section>

    <section class="section" id="1-2-choosing-capstone">
      <h2>1.2 Choosing a capstone domain</h2>
      <p><strong>Why it matters.</strong> The domain determines your <em>dominant constraints</em>. A chat service stresses bidirectional low‑latency streams and presence; an e‑commerce catalog stresses indexing and cache coherency; a metrics pipeline stresses ingestion throughput and time‑series storage. Good domains are simple to explain but rich in trade‑offs.</p>

      <h3 id="1-2-1-example-domains">1.2.1 Example domains</h3>
      <p>Pick one of the following or propose your own. Each comes with characteristic loads and common bottlenecks.</p>
      <ul>
        <li><strong>E‑commerce catalog.</strong> Millions of read‑heavy requests; write bursts during imports; strong need for <em>cache‑aside</em> and search indexing; eventual consistency acceptable for non‑critical metadata.</li>
        <li><strong>Metrics pipeline.</strong> High‑frequency writes (10k–100k events/s); append‑only log (Kafka‑like) feeding stream processors; OLAP queries over columnar storage; strict ordering within partitions.</li>
        <li><strong>Chat service.</strong> Persistent connections (WebSocket/gRPC), fan‑out to subscribers, presence and typing indicators; tolerance for short delays in history sync; abuse protection and rate limiting required.</li>
      </ul>

      <p><strong>Analogy.</strong> Choosing a domain is like picking a sport: the rules shape the training. A marathoner (metrics pipeline) optimizes sustained throughput; a sprinter (chat) optimizes latency; a decathlete (e‑commerce) balances diverse events with transitions (search, browse, checkout).</p>

      <p>Compare: <strong>Capstone domain characteristics</strong></p>
      <table class="table">
        <thead><tr><th>Domain</th><th>Dominant workload</th><th>Primary data store</th><th>Latency target (P95)</th><th>Scaling risk</th></tr></thead>
        <tbody>
          <tr><td>E‑commerce catalog</td><td>Read‑heavy, bursty writes</td><td>RDBMS + cache + search index</td><td>80–150 ms item page</td><td>Cache coherency; index lag</td></tr>
          <tr><td>Metrics pipeline</td><td>Write‑heavy, streaming</td><td>Partitioned log + columnar/time‑series</td><td>&lt; 500 ms ingest</td><td>Hot partitions; backpressure</td></tr>
          <tr><td>Chat service</td><td>Bidirectional, fan‑out</td><td>KV for sessions + message log</td><td>&lt; 150 ms message fan‑out</td><td>Presence scale; thundering herd</td></tr>
        </tbody>
      </table>

      <h3 id="1-2-2-nfr-translation">1.2.2 Translating feature requirements into NFRs</h3>
      <p><strong>Plain.</strong> Turn user stories into measurable constraints. <strong>Formal.</strong> For each critical path, define <em>SLIs</em> (what you measure) and <em>SLOs</em> (the promise), then derive <em>error budgets</em> (allowed non‑compliance) to balance reliability with feature delivery (Google SRE, 2016). <strong>Pitfall.</strong> Teams define SLOs that don’t reflect the user’s experience (e.g., measuring queue depth instead of message delivery time). <strong>Example.</strong> “Messages sent should appear to all recipients within 150 ms for 95% of sends, measured end‑to‑end at the client” is better than “broker latency under 50 ms.”</p>

      <p><strong>Worked example A — E‑commerce item page.</strong> Feature: “Show item details with availability and price.” Derived NFRs: P95 latency ≤ 120 ms from CDN edge; availability ≥ 99.9% monthly; price freshness ≤ 2 min; cost target ≤ $2 per 1k item views. SLIs: <em>edge‑to‑DOMContentLoaded</em>, <em>origin 5xx rate</em>, <em>cache hit ratio</em>. SLOs: 120 ms P95 page render, &lt; 0.1% 5xx at origin, ≥ 85% cache hit ratio. Error budget: 43m 49s per month of SLO non‑compliance at 99.9% availability (Google SRE, 2016).</p>

      <p><strong>Worked example B — Metrics ingestion.</strong> Feature: “Accept product events in real time.” Derived NFRs: sustained ingest 20k events/s with burst to 100k; P95 end‑to‑end ingest &lt; 500 ms; data loss &lt; 0.01% (at‑least‑once); cost ≤ $X/month. SLIs: <em>produce‑to‑commit latency</em>, <em>drop rate</em>, <em>consumer lag</em>. SLOs: 95% &lt; 500 ms; drop rate &lt; 0.01%; consumer lag &lt; 2 minutes. Error budget: you choose to spend it by letting lag grow under heavy load while keeping loss near zero — an explicit trade‑off (Kleppmann, 2017).</p>

      <p><em>Takeaway:</em> If you cannot measure the user‑visible effect of a feature, you haven’t finished designing it.</p>
      <h3 id="1-2-3-nfr-antipatterns">1.2.3 NFR anti‑patterns & how to fix them</h3>
      <p><strong>Anti‑pattern 1 — Vagueness.</strong> “System should be fast.” <strong>Fix:</strong> Tie to a user journey and percentile: “P95 search results in 200 ms for signed‑in users at 100 RPS steady state.”</p>
      <p><strong>Anti‑pattern 2 — Origin‑centric metrics.</strong> “DB CPU under 70%.” <strong>Fix:</strong> Use user‑visible <em>SLIs</em> (time‑to‑first‑byte, end‑to‑end latency) and treat host metrics as diagnostics, not objectives.</p>
      <p><strong>Anti‑pattern 3 — One‑size‑fits‑all SLOs.</strong> Copy‑pasting "five nines" ignores cost and business value. <strong>Fix:</strong> Choose reliability levels by impact (e.g., checkout vs recommendations) and revisit based on error budget burn rate (Google SRE, 2016).</p>
      <p><strong>Anti‑pattern 4 — Unmeasurable SLIs.</strong> If you cannot instrument the client or trace the path, the SLI is aspirational. <strong>Fix:</strong> Add request IDs end‑to‑end, expose client‑side timings, and validate with synthetic probes.</p>
      <p><strong>Anti‑pattern 5 — Hidden freshness requirements.</strong> Many systems care about <em>how fresh</em> data is, not only latency. <strong>Fix:</strong> Add explicit freshness SLIs (e.g., catalog price age ≤ 2 minutes @ P99).</p>
      <p><em>Takeaway:</em> A good NFR is precise, observable, and tied to user value; anything else is a risk you should make visible.</p>
    </section>

    <section class="section" id="1-3-how-to-use">
      <h2>1.3 How to use this book / learning strategy</h2>
      <p><strong>Why it matters.</strong> Architecture is a set of choices under uncertainty. To become effective, you must pair design with implementation, collect measurements, and revise your choices. This chapter equips you with a rhythm: design → implement → observe → critique → iterate.</p>

      <h3 id="1-3-1-paired-learning">1.3.1 Paired learning: design + implementation</h3>
      <p><strong>Plain.</strong> Every concept comes with a small build task. <strong>Formal.</strong> We use <em>deliberate practice</em>: constrained exercises that isolate a capability (e.g., implementing idempotent retries with correlation IDs). <strong>Pitfall.</strong> Reading without building creates the illusion of knowledge; building without measurement creates confidence without evidence. <strong>Example.</strong> After reading about <em>cache‑aside</em> vs <em>write‑through</em>, you implement both on a tiny service, load test for 10 minutes, and document hit ratio versus write amplification.</p>
      <ul>
        <li><strong>Design task:</strong> Draw the critical path for your capstone and mark where you will measure SLIs (latency, availability, correctness).</li>
        <li><strong>Build task:</strong> Instrument a single API endpoint with request tracing; export metrics, logs, and a span that carries a correlation ID end‑to‑end (OpenTelemetry style).</li>
        <li><strong>Measure task:</strong> Run a synthetic test to validate your P95 target; record baseline numbers (<em>users, RPS, P95/P99</em>).</li>
      </ul>

      <p><strong>Compare:</strong> <em>Cache‑aside vs write‑through vs write‑back</em></p>
      <table class="table">
        <thead><tr><th>Pattern</th><th>Pros</th><th>Cons</th><th>When to use</th></tr></thead>
        <tbody>
          <tr><td>Cache‑aside</td><td>Simple; avoids stale writes; low write amplification</td><td>Cold‑start misses; potential thundering herd</td><td>Read‑heavy workloads; tolerate occasional stale reads</td></tr>
          <tr><td>Write‑through</td><td>Cache always warm on write</td><td>Higher write latency; double write path</td><td>Read‑mostly where write latency budget allows</td></tr>
          <tr><td>Write‑back</td><td>Low write latency; batch to origin</td><td>Risk of data loss; complex recovery</td><td>Non‑critical updates; tolerant of temporary inconsistency</td></tr>
        </tbody>
      </table>

      <p><em>Takeaway:</em> You will prefer <em>cache‑aside</em> for the catalog, <em>at‑least‑once write‑through</em> in chat presence, and <em>write‑back</em> only with clear durability guarantees.</p>

      <h3 id=\"1-3-2-reviews-and-postmortems\">1.3.2 Code‑reviews, design critiques, and postmortems</h3>
      <p><strong>Plain.</strong> Reviews create shared ownership and reduce blind spots. <strong>Formal.</strong> We schedule three structured critiques: after data choices (Ch. 3), after reliability planning (Ch. 5), and after pattern selection (Ch. 8). Each ends with an <em>ADR</em> (architecture decision record). <strong>Pitfall.</strong> Treating postmortems as blame hunts discourages reporting; the <em>blameless</em> format (Google SRE) emphasizes learning and concrete follow‑ups.</p>
      <ul>
        <li><strong>Design critique checklist:</strong> Does each service own its data? Do SLOs reflect user perception? Is backpressure defined at <em>every</em> ingress?</li>
        <li><strong>Postmortem minimums:</strong> incident timeline, impact, contributing factors, detection gaps, what went well, specific actions with owners and dates.</li>
        <li><strong>Evidence rule:</strong> no adjectives without numbers; reference graphs or traces when claiming improvement.</li>
        <li><strong>Runbook review:</strong> pick one likely failure per sprint; simulate it; update the runbook with missing steps and automation ideas.</li>
        <li><strong>Decision hygiene:</strong> if a debate lasts over 30 minutes without new information, write an ADR with two options, risks, and a timeboxed experiment.</li>
      </ul>
      <p><em>Takeaway:</em> Critiques tighten the feedback loop; postmortems turn surprises into better runbooks and safer releases.</p>
    </section>

    <section class=\"section\" id=\"1-4-starter-slos\">
      <h2>1.4 Starter SLOs & error budgets</h2>
      <p><strong>Why it matters.</strong> SLOs are your contract with users and your guardrails for change. Error budgets quantify how much unreliability you can tolerate before slowing feature work (Google SRE, 2016).</p>
      <p><strong>Plain.</strong> Choose SLIs that represent user experience (end‑to‑end latency, success rate, freshness). <strong>Formal.</strong> Define a target (e.g., “P95 message fan‑out &lt; 150 ms over 30 days”). The <em>error budget</em> is 1 − SLO (e.g., 0.1% of requests may exceed the target in the window). <strong>Pitfall.</strong> Picking a number without a business reason (e.g., “five nines” by default) increases toil and cost (AWS Well‑Architected, 2024). <strong>Example.</strong> If your checkout requires 99.9% success, that’s 43 minutes of error budget per month; you might choose to spend it during a risky migration behind a feature flag.</p>
      <ul>
        <li><strong>Chat SLO draft:</strong> 95% of messages delivered to all intended recipients within 150 ms over 30 days. <em>SLIs:</em> end‑to‑end send‑to‑receive latency, delivery failure rate, online presence accuracy.</li>
        <li><strong>Catalog SLO draft:</strong> 95% of item page views render within 120 ms at the edge, with correctness for price and availability &gt; 99.99% over 30 days.</li>
        <li><strong>Metrics SLO draft:</strong> 95% of events committed within 500 ms and query freshness &lt; 2 minutes for 99% of queries.</li>
      </ul>
      <p><strong>Worked example C — Chat presence accuracy.</strong> Presence appears simple, but race conditions abound. Define an SLI for <em>presence accuracy</em>: the percentage of users for whom the displayed presence matches server truth within 5 seconds. SLO: ≥ 99.5% over 30 days. <em>Pitfall:</em> measuring only backend heartbeats ignores client tab sleeps and mobile backgrounding. <em>Fix:</em> send periodic client pings with jitter; expire presence conservatively; expose a histogram of client‑side staleness.</p>
      <p><em>Takeaway:</em> Keep SLOs few (2–4 per service), meaningful, and testable. Revisit targets after you have baseline measurements.</p>
    </section>

    <section class=\"section\" id=\"1-5-risks-assumptions\">
      <h2>1.5 Risks, assumptions, and evidence</h2>
      <p><strong>Why it matters.</strong> System design is a negotiation with risk. Making assumptions explicit lets you test them early (e.g., assumed cache hit ratio ≥ 85%). Evidence then updates your plan.</p>
      <p><strong>Plain.</strong> Write down the 3–5 biggest risks and an experiment for each. <strong>Formal.</strong> Use <em>hypothesis → experiment → metric → success threshold</em>. <strong>Pitfall.</strong> Ignoring <em>unknown unknowns</em> — areas with little prior art (e.g., volume of cross‑region traffic). <strong>Example.</strong> “We believe a single Kafka topic with 48 partitions will sustain 20k events/s with P95 &lt; 500 ms; success if consumer lag stays &lt; 2 minutes under a 30‑minute spike.”</p>
      <ul>
        <li><strong>Backpressure drill:</strong> throttle producers when consumer lag &gt; threshold; verify retry jitter prevents synchrony.</li>
        <li><strong>Cache stampede drill:</strong> seed randomized TTLs; enable request coalescing; measure miss storms during deploys.</li>
        <li><strong>Dependency failure drill:</strong> add circuit breakers and timeouts; confirm graceful degradation UX.</li>
      </ul>
      <p><strong>Risk register template:</strong></p>
      <table class=\"table\">
        <thead><tr><th>Risk</th><th>Assumption</th><th>Experiment</th><th>Metric</th><th>Exit criteria</th></tr></thead>
        <tbody>
          <tr><td>Hot partition</td><td>Uniform key distribution</td><td>Replay 10× hottest key</td><td>P95 latency; CPU</td><td>P95 &lt; 2× baseline; CPU &lt; 75%</td></tr>
          <tr><td>Cache stampede</td><td>Warmed cache</td><td>Invalidate 5% hot keys</td><td>Origin QPS; miss ratio</td><td>Origin QPS spike &lt; 1.5×; miss ratio recovers &lt; 60s</td></tr>
          <tr><td>Downstream slowness</td><td>99% &lt; threshold</td><td>Inject +200 ms latency</td><td>Timeout rate; user SLI</td><td>Timeout &lt; 1%; SLI holds</td></tr>
        </tbody>
      </table>
      <p><em>Takeaway:</em> Turn fears into tests; record outcomes in the design log and evolve your architecture.</p>
    </section>

    <section class=\"section\" id=\"1-6-mini-exercises\">
      <h2>1.6 Applied mini‑exercises</h2>
      <ol>
        <li><strong>Service boundary sketch.</strong> Draw three candidate boundaries for your domain. For each, list one advantage and one risk (e.g., data ownership vs cross‑service transactions). Choose one and justify. <em>Acceptance:</em> a diagram, a paragraph per option, and an ADR documenting the decision and revisit trigger.</li>
        <li><strong>First SLO draft.</strong> Define one latency SLI and one availability SLI for your core user journey. Explain why they reflect user experience. <em>Acceptance:</em> an SLI definition (units, percentile, window), a target, and how you’ll measure it in staging.</li>
        <li><strong>Back‑of‑envelope capacity.</strong> Estimate peak QPS, bandwidth, and storage growth for 6 months. Document the margin (e.g., 30%) and the signal you’ll use to re‑estimate. <em>Acceptance:</em> a short table with inputs, assumptions, and a graph screenshot after a 10‑minute synthetic test.</li>
      </ol>
      <p><em>Takeaway:</em> Small designs plus small builds compound into confidence for the capstone.</p>
    </section>

    <section class="section" id="1-7-resources">
      <h2>Resources</h2>
      <ul>
        <li><strong>Designing Data‑Intensive Applications</strong> — authoritative on data trade‑offs, consistency models, and streaming vs batch (O’Reilly, 2017/2025 update). <em>Why:</em> clarifies <em>when</em> to choose a tool (Kleppmann). <em>(Paid)</em></li>
        <li><strong>Martin Fowler’s Microservices Guide</strong> — service boundaries, testing strategies, and evolutionary design. <em>Why:</em> avoids premature decomposition. <em>(Free)</em></li>
        <li><strong>Google SRE Book & Site</strong> — SLIs/SLOs, error budgets, and incident practices. <em>Why:</em> ties reliability to user value with concrete methods. <em>(Free)</em></li>
        <li><strong>AWS Well‑Architected Framework</strong> — decision lenses for reliability, performance, cost, and operations. <em>Why:</em> prompts you to ask the right questions before you build. <em>(Free)</em></li>
        <li><strong>Kubernetes Documentation</strong> — orchestration fundamentals for Chapter 7. <em>Why:</em> helps you reason about cluster‑level trade‑offs. <em>(Free)</em></li>
      </ul>
      <p class="muted">Citations: (Kleppmann, O’Reilly), (Fowler, martinfowler.com), (Google SRE site & book), (AWS Docs, 2024), (Kubernetes Docs).</p>
    </section>

    <section class="section" id="1-8-recap-next">
      <h2>Recap & Next Steps</h2>
      <ul>
        <li>You set a realistic cadence and milestones, emphasizing observability and reliability alongside features.</li>
        <li>You picked or narrowed down a capstone domain and translated features into measurable NFRs.</li>
        <li>You drafted user‑centric SLOs and identified the first risk experiments to run.</li>
        <li>You scheduled design critiques and understood how postmortems turn incidents into improvements.</li>
      </ul>
      <p><strong>Next:</strong> Proceed to <a href="chapters/ch02.html#2-hero">Chapter 2 — System Building Blocks</a>, where you’ll map requirements to scalable components and justify choices with trade‑offs.</p>
    </section>

    <section class="section" id="image-audit">
      <h2>Image Link Audit</h2>
      <ul>
        <li><code>https://www.oreilly.com/covers/urn%3Aorm%3Abook%3A9781491903063/400w/</code> — Status: 200 OK; Content‑Type: image/jpeg; Validation: <strong>PASS</strong>.</li>
      </ul>
    </section>

    <nav class="next-prev">
      <a class="btn" rel="prev" aria-disabled="true" tabindex="-1">← Previous</a>
      <a class="btn btn-primary" rel="next" href="chapters/ch02.html">Next →</a>
    </nav>

    <footer class="site-footer">
      <div class="container">
        <p class="muted">© 2025 BookBuilder. Built with vanilla HTML/CSS/JS. Dark theme.</p>
      </div>
    </footer>
  </main>

  <!--
  CHECKLIST (Chapter 01)
  - [x] /styles/theme.css + /scripts/app.js linked; <base> correct; no inline nav JS
  - [x] Canonical nav (Home / Appendix / Glossary only)
  - [x] Pager prev/next valid (no prev; next → ch02.html)
  - [x] Order: Hero → Numbered Sections → Resources → Recap
  - [x] ≥1,800 words of prose (body text, including lists/tables/captions)
  - [x] Head/meta complete; no TODOs
  - [x] Images: HTTPS, non‑SVG, 200 OK, image/*, required attrs + caption + attribution
  -->
</body>
</html>

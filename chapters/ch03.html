<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Chapter 03 — Data & Storage Patterns for Scale</title>
  <meta name="description" content="Choose storage engines by workload, design partitioning and sharding, plan indexing and denormalization, and understand analytics vs streaming trade-offs.">
  <meta property="og:title" content="Chapter 03 — Data & Storage Patterns for Scale">
  <meta property="og:description" content="Storage types and selection criteria, partitioning strategies, indexing and denormalization, and analytics & streaming design patterns.">
  <meta property="og:type" content="article">
  <base href="../">
  <link rel="stylesheet" href="styles/theme.css">
  <script src="scripts/app.js" defer></script>
</head>
<body>
  <a class="skip-link" href="#main">Skip to main content</a>

  <header class="app-nav">
    <div class="container inner">
      <div class="brand">Intermediate System Design</div>
      <button class="toggle js-nav-toggle" aria-expanded="false" aria-controls="top-menu">☰ Menu</button>
      <nav id="top-menu" class="menu" aria-label="Primary">
        <a href="index.html">Home</a>
        <a href="chapters/appendix.html">Appendix</a>
        <a href="chapters/glossary.html">Glossary</a>
      </nav>
    </div>
  </header>

  <main id="main" class="container fade-in">
    <section class="page-hero" id="3-hero">
      <div class="meta"><span class="badge badge-primary">Chapter 03</span></div>
      <h1>Data & Storage Patterns for Scale</h1>
      <p class="abstract">Storage choices multiply as systems grow: relational OLTP for correctness, schemaless stores for flexibility, columnar engines for analytics, time‑series for metrics, search indexes for discovery, and object storage for blobs. This chapter shows how to map workloads to stores, avoid hot partitions, design indices that fit read patterns, and balance normalization with denormalization. We end with streaming vs batch decisions and how to connect operational and analytic worlds without creating brittle data plumbing.</p>
    </section>

    <section class="section" id="3-1-storage-selection">
      <h2>3.1 Storage types & selection criteria</h2>
      <p><strong>Why it matters.</strong> The wrong store quietly taxes every feature: queries stay slow, migrations drag on, and reliability erodes under load. Selecting by <em>workload</em> rather than fashion is the fastest way to keep both cost and complexity in check.</p>

      <h3 id="3-1-1-oltp-olap-streaming">3.1.1 OLTP vs OLAP vs streaming</h3>
      <p><strong>Plain.</strong> OLTP systems handle user transactions (reads/writes in small chunks) and demand low latency and strong integrity. OLAP systems crunch large datasets for insights and favor scans and aggregations. Streaming adds continuously arriving, ordered events processed with low latency. <strong>Formal.</strong> OLTP optimizes for point reads/writes and transactional guarantees (ACID). OLAP engines (often columnar) optimize for vectorized scans and aggregations across many rows but few columns, with looser write semantics. Streaming engines process unbounded logs with windowing semantics and exactly‑once or at‑least‑once delivery guarantees at the framework level. <strong>Pitfall.</strong> Forcing analytics onto your OLTP database degrades both: operational queries become slow and analysts circumvent safeguards.</p>

      <p>Compare: <strong>OLTP vs OLAP vs Streaming</strong></p>
      <table class="table">
        <thead><tr><th>Axis</th><th>OLTP (relational)</th><th>OLAP (columnar)</th><th>Streaming (log + processor)</th></tr></thead>
        <tbody>
          <tr><td>Typical latency</td><td>1–20 ms</td><td>Seconds to minutes</td><td>10–1000 ms</td></tr>
          <tr><td>Data shape</td><td>Highly normalized rows</td><td>Wide, columnar tables</td><td>Append‑only events</td></tr>
          <tr><td>Access pattern</td><td>Point read/write</td><td>Scans, group‑by, joins</td><td>Per‑key aggregation, window ops</td></tr>
          <tr><td>Consistency</td><td>Strong (ACID)</td><td>Batch/append with isolation</td><td>Eventual per partition</td></tr>
          <tr><td>Best for</td><td>Orders, users, inventory</td><td>Dashboards, BI, ML features</td><td>Metrics, feeds, CDC pipelines</td></tr>
        </tbody>
      </table>

      <h3 id="3-1-2-when-object-columnar-tsdb">3.1.2 When to use object storage, columnar stores, time‑series DBs</h3>
      <p><strong>Plain.</strong> Object storage is your durable, cheap “data lake” for blobs and immutable files; columnar stores provide fast analytics; time‑series databases simplify metric and event retention. <strong>Formal.</strong> Object stores expose eventual consistency (increasingly strong for some operations), lifecycle policies, and tiered pricing. Columnar engines compress by column and exploit vectorized execution. TSDBs optimize for append, down‑sampling, and time‑based retention.</p>
      <ul>
        <li><strong>Object storage:</strong> product images, exports, model artifacts, raw logs; pair with a CDN for distribution. Plan for content hashes and immutable URLs to simplify cache busting.</li>
        <li><strong>Columnar OLAP:</strong> event analytics, cohorts, dashboards; batch ingest or micro‑batches from a stream; expect seconds‑level query times.</li>
        <li><strong>Time‑series:</strong> service metrics, IoT readings, financial ticks; prioritize cardinality control and retention policies to predict storage cost.</li>
      </ul>
      <p><em>Takeaway:</em> Separate concerns: OLTP for correctness, OLAP for insight, streaming for timeliness. Link them with reproducible data flows, not ad‑hoc copies.</p>
    </section>

    <section class="section" id="3-2-partitioning">
      <h2>3.2 Partitioning & sharding</h2>
      <p><strong>Why it matters.</strong> Vertical scale runs out. Partitioning (splitting data across nodes) is how you grow beyond a single machine and avoid noisy‑neighbor effects. Do it thoughtfully and you get linear scale; do it naively and you get hot partitions and outages.</p>

      <h3 id="3-2-1-horizontal-vertical">3.2.1 Horizontal vs vertical partitioning</h3>
      <p><strong>Plain.</strong> Horizontal partitioning (sharding) splits rows by key; vertical partitioning splits columns by access pattern or sensitivity. <strong>Formal.</strong> For sharding, a deterministic function maps a key to a shard: <em>shard = hash(key) mod N</em> or via consistent hashing that limits rebalancing when <em>N</em> changes. Vertical partitioning stores volatile or large columns separately to improve cache locality and reduce IO on hot paths. <strong>Pitfall.</strong> Using an ever‑increasing key (e.g., timestamp) concentrates writes on one shard; mixing transactional and blob columns bloats working sets.</p>

      <p><strong>Worked example — Hash vs range vs directory sharding.</strong> A catalog with item_id lookups can hash on <em>item_id</em> for even spread; a metrics pipeline that queries time windows prefers range partitions by time; a multi‑tenant SaaS that needs data isolation uses directory‑based sharding (a mapping table from tenant → shard). <em>Trade‑off:</em> hashing gives balance but complicates range scans; ranges give locality but risk hot shards; directories add flexibility but require an always‑consistent mapping service.</p>

      <h3 id="3-2-2-hot-partitions">3.2.2 Hot‑partition mitigation strategies</h3>
      <p><strong>Plain.</strong> Hot partitions occur when a small set of keys attract disproportionate traffic. <strong>Formal.</strong> The 99th percentile key may receive 10–100× the median; load must be spread in time or space. <strong>Pitfall.</strong> Key design is often chosen once and forgotten; revisit it as traffic evolves.</p>
      <ul>
        <li><strong>Key randomization:</strong> add a small prefix/suffix bucket to distribute writes (e.g., 0–9 + user_id) and read from all buckets when fetching. Good for counters and logs.</li>
        <li><strong>Write buffering:</strong> accumulate writes in a log, then batch/merge to the base table to smooth spikes (common for counters).</li>
        <li><strong>Time bucketing:</strong> for time‑ordered data, split by time windows (hour/day) to limit write contention and compact cold partitions.</li>
        <li><strong>Read‑replicas & fan‑out:</strong> for hot reads, replicate and use request coalescing to avoid stampedes.</li>
        <li><strong>Adaptive rebalancing:</strong> with consistent hashing, increase virtual nodes for hot tenants; with directory sharding, move the tenant to a larger shard group.</li>
      </ul>
      <p><em>Takeaway:</em> Treat key choice as a performance feature; instrument per‑key skew and act before it pages you.</p>
    </section>

    <section class="section" id="3-3-indexing-denorm">
      <h2>3.3 Indexing, secondary indices & denormalization</h2>
      <p><strong>Why it matters.</strong> Most production latency problems stem from mismatched access patterns and indices. The right index often beats adding more hardware. The right denormalization avoids expensive fan‑out queries at scale.</p>

      <h3 id="3-3-1-read-patterns">3.3.1 Read patterns and denormalization trade‑offs</h3>
      <p><strong>Plain.</strong> Indexes accelerate reads by maintaining a structure keyed by columns; denormalization stores pre‑joined or aggregated data for common queries. <strong>Formal.</strong> A B‑tree index maintains sorted keys for logarithmic lookup; a hash index gives O(1) equality but no range order. Denormalization increases write amplification and risks staleness; design invalidation or refresh mechanisms up front. <strong>Pitfall.</strong> Adding many indexes speeds some reads but slows every write and complicates recovery.</p>

      <p>Compare: <strong>Index & denormalization options</strong></p>
      <table class="table">
        <thead><tr><th>Technique</th><th>Helps</th><th>Costs</th><th>Use when</th></tr></thead>
        <tbody>
          <tr><td>Composite index (A, B)</td><td>Filters on A and B; ORDER BY B</td><td>Extra space; maintain on write</td><td>Frequent query on (tenant_id, created_at)</td></tr>
          <tr><td>Covering index</td><td>Query served from index alone</td><td>Wider index = more memory</td><td>Hot endpoints with fixed projections</td></tr>
          <tr><td>Partial index</td><td>Ignore cold rows</td><td>Query planner complexity</td><td>Predicates like active=true</td></tr>
          <tr><td>Materialized view</td><td>Pre‑aggregated results</td><td>Refresh cost; staleness</td><td>Dashboards; fan‑out reads</td></tr>
          <tr><td>Pre‑joined table</td><td>One read instead of many</td><td>Duplication; incoherency risk</td><td>Latency‑critical endpoints</td></tr>
        </tbody>
      </table>

      <p><strong>Worked example — Product page.</strong> Endpoint reads: product → seller → price → inventory. Instead of four joins per request, create a <em>product_view</em> table containing denormalized fields and a <em>last_refreshed_at</em> column. Updates flow via change events from source tables to refresh the view. Latency drops; write cost rises. <em>Mitigation:</em> batch/schedule refresh for non‑critical fields (e.g., description) and use immediate updates for price/stock.</p>

      <h3 id="3-3-2-materialized-eventual">3.3.2 Materialized views and eventual consistency</h3>
      <p><strong>Plain.</strong> If you precompute, you accept staleness. <strong>Formal.</strong> Define freshness SLIs (e.g., “P99 product_view age ≤ 120 seconds”). <strong>Pitfall.</strong> Lacking a backfill path leads to permanent gaps after outages. <strong>Example.</strong> Maintain a durable event log (CDC from OLTP); processors rebuild the view by replay. Keep idempotent updates keyed by primary key + version.</p>

      <pre data-lang="sql"><code>-- Example: materialized view refresh marker
CREATE TABLE product_view (
  product_id     BIGINT PRIMARY KEY,
  seller_id      BIGINT,
  price_cents    BIGINT,
  inventory_qty  INT,
  last_refreshed_at TIMESTAMP NOT NULL
);

-- CDC consumer (pseudocode):
-- on event e for product_id:
--   read current source rows
--   upsert into product_view with refreshed fields and now() as last_refreshed_at</code></pre>

      <p><em>Takeaway:</em> With denormalization, your job is not to eliminate staleness but to bound and communicate it.</p>
    </section>

    <section class="section" id="3-4-analytics-streaming">
      <h2>3.4 Storage for analytics & streaming</h2>
      <p><strong>Why it matters.</strong> Many teams need both dashboards and fresh features (recommendations, fraud detection). Bridging streaming and batch without duplication avoids two sources of truth.</p>

      <h3 id="3-4-1-kafka-vs-batch">3.4.1 Kafka‑like patterns vs batch ETL</h3>
      <p><strong>Plain.</strong> A log of immutable events decouples producers and consumers; consumers build their own state. Batch ETL collects data into files and transforms them periodically. <strong>Formal.</strong> Streaming favors low latency and incremental state; batch favors throughput and simplified correctness with full reprocessing. <strong>Pitfall.</strong> Over‑streaming: pushing low‑value events through expensive real‑time paths; over‑batching: dashboards and ML features too stale for the application.</p>

      <ul>
        <li><strong>When to stream:</strong> user‑visible freshness under a few minutes; online features; operational alerting; incremental ML features.</li>
        <li><strong>When to batch:</strong> heavy transforms; cost‑sensitive backfills; daily/weekly reporting; long historical joins.</li>
        <li><strong>Hybrid:</strong> stream to serve operational needs; land to object storage for batch backfills and training.</li>
      </ul>

      <p><strong>Worked example — Real‑time metrics with OLAP sink.</strong> Ingest events to a partitioned log; a streaming job aggregates per key and writes (1) low‑latency counters to a TSDB for dashboards and alerts, and (2) hourly parquet files to object storage for a columnar warehouse. This yields both quick graphs and cheap historical analytics. <em>Trade‑off:</em> duplicate pipelines; mitigate with shared schemas and a governance process.</p>

      <h3 id="3-4-2-event-sourcing-vs-cdc">3.4.2 Event‑sourcing vs change‑data‑capture (CDC)</h3>
      <p><strong>Plain.</strong> Event sourcing stores the sequence of domain events as the truth and rebuilds state from them; CDC taps changes from an OLTP database and forwards them elsewhere. <strong>Formal.</strong> Event sourcing requires event design and versioning; CDC mirrors mutations with minimal app changes. <strong>Pitfall.</strong> Using event sourcing only to feed analytics complicates the write path without clear benefit; using CDC without schema discipline spawns schema‑drift problems downstream.</p>
      <ul>
        <li><strong>Use event sourcing</strong> when events are first‑class (auditing, replay requirements) and you can invest in event schemas and tooling.</li>
        <li><strong>Use CDC</strong> when OLTP remains the source of truth but you need read models, caches, or analytics mirrors.</li>
        <li><strong>Both</strong> when some domains are event‑native (payments) and others are CRUD; don’t force a single pattern everywhere.</li>
      </ul>
      <p><em>Takeaway:</em> Choose the <em>simplest</em> mechanism that preserves required history and enables downstream uses.</p>
    </section>

    <section class="section" id="3-5-applied">
      <h2>3.5 Applied scenarios & calculations</h2>
      <p><strong>Scenario A — Sizing a partitioned log.</strong> You expect 20k events/s steady, spikes to 100k. With per‑partition throughput ≈ 5k events/s, start with 32–48 partitions to keep P95 commit latency under your 500 ms target. Reserve headroom (≈ 30%) for retries and consumer lag recovery. Monitor the <em>producer‑to‑commit latency</em> and <em>consumer lag</em> SLIs; adjust partitions or consumer parallelism based on burn‑in data.</p>
      <p><strong>Scenario B — Hot‑key catalog.</strong> A celebrity seller lists items causing a key skew: 30% of reads for 0.1% of items. Add a <em>read‑through cache</em> with request coalescing and pre‑warming; move to <em>composite key</em> buckets for write distribution. If miss storms persist, add <em>per‑item rate limiting</em> and a fallback to static pages.</p>
      <p><strong>Scenario C — Denormalized product view freshness.</strong> Suppose your SLO: P99 <em>product_view</em> freshness ≤ 120 s. You process CDC with a stream job; under backlog, freshness burns error budget. Set a <em>graceful degradation</em>: expose a “price may be updating” banner if staleness &gt; 2 min and block checkout on stale price for regulated regions.</p>
      <p><strong>Scenario D — Time‑series cardinality control.</strong> Your TSDB stores metrics with tags (service, endpoint, status). Adding user_id explodes cardinality. Establish <em>tag hygiene</em>: only high‑cardinality dimensions go to logs or sampled traces; metrics keep bounded labels. Use metrics exemplars to link to traces for high‑cardinality drill‑downs.</p>
    </section>

    <section class="section" id="3-6-mini-exercises">
      <h2>3.6 Mini‑exercises</h2>
      <ol>
        <li><strong>Workload matrix:</strong> For your capstone, list three core queries and three core writes. For each, choose a store (OLTP, search, TSDB, object, columnar) and justify with latency and consistency needs.</li>
        <li><strong>Shard key design:</strong> Propose a primary shard key and two alternatives. For each, analyze hot‑key risk and the cost of moving to the alternative later.</li>
        <li><strong>Index plan:</strong> For your two hottest endpoints, propose indices (including composite/covering). Estimate write amplification and memory overhead and set a review trigger (e.g., when table reaches 100M rows).</li>
        <li><strong>Freshness SLI:</strong> Define a freshness SLI and target for one denormalized view. Decide whether to reprocess on demand or lazily, and document how users will experience staleness.</li>
      </ol>
    </section>

    <section class="section" id="3-7-resources">
      <h2>Resources</h2>
      <ul>
        <li><strong>Data system fundamentals (book)</strong> — clears up transactional vs analytical trade‑offs and consistency models. <em>Why:</em> decide <em>when</em> to denormalize and how to bound staleness. <em>(Paid)</em></li>
        <li><strong>Microservices integration essays</strong> — practical guidance for using read models and avoiding distributed transactions. <em>Why:</em> safer ways to let services own their data. <em>(Free)</em></li>
        <li><strong>SRE guidance on SLIs/SLOs</strong> — model freshness/error budgets for data pipelines and views. <em>Why:</em> keeps user value front‑and‑center. <em>(Free)</em></li>
        <li><strong>Cloud architecture lenses</strong> — cost, reliability, and performance questions to ask before choosing a store. <em>Why:</em> forces decision hygiene. <em>(Free)</em></li>
      </ul>
    </section>

    <section class="section" id="3-8-recap-next">
      <h2>Recap & Next Steps</h2>
      <ul>
        <li>You can select stores by workload rather than brand: OLTP for correctness, OLAP for insights, TSDB for metrics, search for discovery, object storage for blobs.</li>
        <li>You can design partitioning schemes that avoid hot shards and have a concrete playbook for skew and rebalancing.</li>
        <li>You can map read patterns to indices and decide when to denormalize, with freshness SLIs to bound staleness.</li>
        <li>You can connect streaming and batch without creating two sources of truth, and you know when to use CDC vs event sourcing.</li>
      </ul>
      <p><strong>Next:</strong> Proceed to <a href="chapters/ch04.html#4-hero">Chapter 4 — Microservices, Boundaries &amp; Integration</a>, where we use domain‑driven boundaries and integration patterns to connect your stores and services into a coherent whole.</p>
    </section>

    <nav class="next-prev">
      <a class="btn" rel="prev" href="chapters/ch02.html">← Previous</a>
      <a class="btn btn-primary" rel="next" href="chapters/ch04.html">Next →</a>
    </nav>

    <footer class="site-footer">
      <div class="container">
        <p class="muted">© 2025 BookBuilder. Built with vanilla HTML/CSS/JS. Dark theme.</p>
      </div>
    </footer>
  </main>

  <!--
  CHECKLIST (Chapter 03)
  - [x] /styles/theme.css + /scripts/app.js linked; <base> correct; no inline nav JS
  - [x] Canonical nav (Home / Appendix / Glossary only)
  - [x] Pager prev → ch02, next → ch04; ToC numbering matches
  - [x] Order: Hero → Numbered Sections → Resources → Recap
  - [x] ≥1,800 words of prose (body text)
  - [x] Images: none (no audit required)
  - [x] Head/meta complete; no TODOs
  -->
</body>
</html>
